{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_TicTacToe_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AO0tWLLZnHm1",
        "ECY9uCdusn2C",
        "SiFkHSUvsMPb"
      ],
      "authorship_tag": "ABX9TyMaEX/QKkrROF8c4lH+B9EO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inforeqd512/QLearning/blob/main/RL_TicTacToe_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09zQDJ8boz15"
      },
      "source": [
        "import numpy as np\n",
        "import pylab as plt"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhMbEyXuXjS7"
      },
      "source": [
        "# Game\n",
        "\n",
        "Every simulation starts with a new Game _Board_. \n",
        "\n",
        "Each iteration of the simulation ends when the board is full or when any of the players wins.\n",
        "\n",
        "Every Player _Agent_ is setup with their _Policy Trainer_ and follows espilon greedy policy to learn best actions over several iterations of the simulation.\n",
        "\n",
        "**measure of success** : The Players become good at playing when the 'Draw' percentages are high.. ie. the first player to go learns to play in the middle or lears to play to Draw\n",
        " \n",
        "It takes approximately 5000 iterations to get to optimal q-values. At that time both players learn the game well enough to win approx 50% of the time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs_GTXqantDk"
      },
      "source": [
        "### Actors in the Game"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaQuOM1Mnu90"
      },
      "source": [
        "\n",
        "_Board_ is the environment. _State_ is the position of the board after a move from any player. Board also provides the possible set of actions. These are set of open positions on the board on which the player can put it's symbol. \n",
        "\n",
        "_Agent_ is the player playing the game. \n",
        "\n",
        "- Player playing $X$ will be denoted by symbol $1$\n",
        "- Player playing $O$ will be denoted by symbol $-1$\n",
        "\n",
        "Every _Agent_ has its own _Policy Trainer_ that learns the best action it should do in the _Board_ environment. \n",
        "\n",
        "The _Policy_Trainer_ follows is the epsilon greedy random policy to choose the next action. Based on this policy it learns the _Quality Action or Q-table_ for each player. Typically the Q-values of Player X are different to those of Player O. \n",
        "\n",
        "The _Policy_Trainer_ also needs to update the Q-values after performing the action and seeing the reward, so the _Agent_ delegates the choosing of the action and the performing of the action to it. \n",
        "\n",
        "\n",
        "_Action_ to be performed is chosen by the agent on the current state of the board. It is the next available position on the board that the agent can choose from. \n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKTADaJ5V2Wh"
      },
      "source": [
        "### Exploration and Exploitation\n",
        "\n",
        "Initially we explore more as we learn about the possible moves that can be made. Over multiple iterations we start exploiting what we've learnt so we decay the exploration factor epsilon. \n",
        "\n",
        "**Decaying epsilon-greedy**: Does the same as epsilon-greedy, however, the epsilon value starts out near 1, and decays over time according to γ to power of x where x represents the iteration the agent is in.  The below article recommends γ = 0.99 \n",
        "\n",
        "<sup>Source: [The Effect of the Exploration\n",
        "Strategy on an Agent’s Performance](https://theses.ubn.ru.nl/bitstream/handle/123456789/5216/Nieuwdorp,%20T._BSc_Thesis_2017.pdf?sequence=1) from Thijs Nieuwdorp</sup>\n",
        "\n",
        "However, on trying out several values, it became clear that to reach the [Nash Equilibrium](https://www.investopedia.com/terms/n/nash-equilibrium.asp) we need enough iterations to first have an exploration of the states and then it needs to settle down to apply the policy and improve it... so with 0.95 and allowing epsilon to decay till 0.001 it gives the two policy system around 100 sets of iterations for exploration before settling down and improving. That way both players improve till the last set of games are draws.\n",
        "\n",
        "Another way to improve this logic is to start the game not just on empty board, but say with different states. eg start with X at the center of the board,  different legal states of the board.... so that the value of the states are computed from then on... This way it may create better policy for when X wins because of O's mistakes due to explorations rather than sticking to their best policy... \n",
        "\n",
        "**When to decay**\n",
        "Strategy is to play 200 sets of 50 games each.. Run the 50 games with the same epsilon decay value so that it is used for maximum exploration. Then decay it after every set of 50 so that over 100 sets the policy has done maximum exploration. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWBNNhH_Veiz"
      },
      "source": [
        "### Quality of Action \n",
        "\n",
        "**Q (dict)** - \n",
        "\n",
        "- key = state hash, \n",
        "- value = is an array representing 9 spaces in the tic tac toe grid,\n",
        "\n",
        "Starting from top row of grid, from left to right then next row from left to right etc, based on the grid position sent as input action, the array index is computed\n",
        "      \n",
        "eg \n",
        "- for 0,1 grid position = 0 * 3 (number of rows and cols in grid) + 1 == 1 so the index of 1.. ie at position 2 in the array\n",
        "- for 1,1 grid position = 1 * 3 + 1 == 4 so 0,1,2,3,4th position in the array \n",
        "\n",
        "beware!!! when using the default initialisation of parameter as {} then python allocates the same memory to both instances of PolicyTrainer. As a result both policies showed the same scores. Also showed win for 1 when should show only for -1.. The best way to find this happening is to initialise the game state to a board where only one player can win, but you'll see that both players get a non-zero Q-value\n",
        "\n",
        "**Initialising Q-table**\n",
        "The Q-table is cross product of 'states' X 'valid actions in state'.  in any board state, the positions that are already filled by X/O are illegal and will have no valid Q values that should contribute to the next maxQ. Therefore these illegal states are initialised to -2 which is lower than the reward for losing.. It could also be initialised to -ve infinity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO0tWLLZnHm1"
      },
      "source": [
        "# Board"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtDnMMJ_nHP2"
      },
      "source": [
        "class Board:\n",
        "  \"\"\" Class that represents the game board of Tic Tac Toe \"\"\"\n",
        "\n",
        "  playerX = 1\n",
        "  playerO = -1\n",
        "\n",
        "  def __init__(self, rows=3, cols=3):\n",
        "    self.rows = rows\n",
        "    self.cols = cols \n",
        "    self.state = np.zeros((self.rows, self.cols), dtype=np.int8)\n",
        "    #TODO: board states can be reset to different legal intermediate states \n",
        "    #for a certain number of iterations instead of always starting from the beginning\n",
        "    # self.state = np.array(\n",
        "    #               [[0,  0, 0],\n",
        "    #                [0,  1,  0],\n",
        "    #                [0, 0, 0]])\n",
        "    \n",
        "  def showBoard(self):\n",
        "    \"\"\"  displays the board with X/O that were placed\n",
        "    useful when wanting to see how the games are being played\"\"\"\n",
        "    for i in range(0, self.rows):\n",
        "        print('-------------')\n",
        "        out = '| '\n",
        "        for j in range(0, self.cols):\n",
        "            if self.state[i, j] == 1:\n",
        "                token = 'x'\n",
        "            if self.state[i, j] == -1:\n",
        "                token = 'o'\n",
        "            if self.state[i, j] == 0:\n",
        "                token = ' '\n",
        "            out += token + ' | '\n",
        "        print(out)\n",
        "    print('-------------')\n",
        "\n",
        "\n",
        "  def checkWinner(self):\n",
        "    \"\"\"  return winner symbol, if one exists. 0 if no winner\"\"\"\n",
        "\n",
        "    symbols = np.unique(self.state) #unique values , 0, 1, -1\n",
        "    symbols = symbols[np.nonzero(symbols)] #remove 0's\n",
        "    winning_symbol = 0 #no winner yet\n",
        "\n",
        "    for symbol in symbols:\n",
        "      #check rows\n",
        "      row = np.any(np.all(self.state == symbol, axis=1))\n",
        "\n",
        "      #check cols\n",
        "      col = np.any(np.all(self.state == symbol, axis=0))\n",
        "\n",
        "      #check diagonals\n",
        "      diag1 = np.array([self.state[0,0], self.state[1,1], self.state[2,2]])\n",
        "      diag1 = np.all(diag1 == symbol)\n",
        "\n",
        "      diag2 = np.array([self.state[2,0], self.state[1,1], self.state[0,2]])\n",
        "      diag2 = np.all(diag2 == symbol)\n",
        "\n",
        "      # Check if state has winner and return winner in that case\n",
        "      if row or col or diag1 or diag2:\n",
        "        winning_symbol = symbol\n",
        "        break\n",
        "  \n",
        "    return winning_symbol\n",
        "\n",
        "  def getAvailablePos(self):\n",
        "    \"\"\"  Get state positions that have no value ie zeros \"\"\"\n",
        "    return np.argwhere(self.state == 0)\n",
        "\n",
        "  def isBoardFull(self):\n",
        "    \"\"\"  checks whether all positions in board has been filled \"\"\"\n",
        "    return (len(self.getAvailablePos()) == 0)\n",
        "\n",
        "  def checkGameEnded(self):\n",
        "    \"\"\" Check if game has ended by observing if there any possible moves left\n",
        "    or there is a winner \"\"\"\n",
        "    ended = (self.isBoardFull()) or (self.checkWinner() != 0)\n",
        "    return ended\n",
        "\n",
        "  def setPosition(self, x, y, symbol):\n",
        "    \"\"\"  Set state at position (x,y) with symbol \"\"\"\n",
        "    self.state[x,y] = symbol\n",
        "\n",
        "  def getStateHash(self):\n",
        "    \"\"\"  Get hash key of state \"\"\"\n",
        "    boardHash = str(self.state.reshape(self.rows * self.cols))\n",
        "    return boardHash\n",
        "  \n",
        "  def performAction(self, action_to_perform, symbol):\n",
        "    \"\"\"  Perform the action on current board \"\"\"\n",
        "    self.setPosition(action_to_perform[0], action_to_perform[1], symbol)\n",
        "    return self\n",
        "      "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECY9uCdusn2C"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aWLkACpzJ1v"
      },
      "source": [
        "EPSILON_DECAY_VALUE = 0.97"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSWPabSDToIi"
      },
      "source": [
        "class Agent:\n",
        "  \"\"\" Class that represents the player \n",
        "      symbol is 1 for 'X' or -1 for 'O' \n",
        "      policy_trainer is initialised with epsilon greedy set to exploration_probability which is reduced overtime\"\"\"\n",
        "\n",
        "  def __init__(self, symbol, exploration_probability):\n",
        "    self.symbol = symbol\n",
        "    self.policy_trainer = PolicyTrainer(exploration_probability=exploration_probability, symbol=symbol)\n",
        "    return\n",
        "\n",
        "  def performActionPerPolicy(self, state_hash, possible_actions, current_state):\n",
        "    \"\"\" per the explore and exploitation with current epsilon \"\"\"\n",
        "    self.policy_trainer.initQValues(state_hash, possible_actions)\n",
        "    next_action = self.policy_trainer.chooseAction(state_hash, possible_actions)\n",
        "    self.policy_trainer.performAction(current_state, next_action)\n",
        "\n",
        "  def applyFinalResult(self, game_board):\n",
        "    \"\"\" Gets called after the game has finished. Will update the current Q function based on the game outcome.\n",
        "    \"\"\"\n",
        "    self.policy_trainer.applyFinalResult(game_board)\n",
        "\n",
        "  def epsilonDecayPerIterations(self, num_iterations):\n",
        "    \"\"\" gradually reduces probabilities per iteration but not completely eliminate it\"\"\"\n",
        "    # Reduce probability to explore during training\n",
        "    # Do not remove completely \n",
        "    \n",
        "    # Decaying epsilon-greedy: Does the same as epsilon-greedy, however, the epsilon value starts out near 1,\n",
        "    # and decays over time according to γ to power of x where x represents the iteration the agent is in. \n",
        "    # For γ 0.99 is used per https://theses.ubn.ru.nl/bitstream/handle/123456789/5216/Nieuwdorp,%20T._BSc_Thesis_2017.pdf?sequence=1\n",
        "    \n",
        "    #noticed that for TIC TAC TOE, as iterations progress the agent should start only using the best policy and not do any more exploration\n",
        "    #so that it always playes the best game as time goes by and leads to more draws\n",
        "    if self.policy_trainer.exploration_probability > 0.001: \n",
        "      decay = EPSILON_DECAY_VALUE ** num_iterations \n",
        "      self.policy_trainer.exploration_probability = decay\n",
        "      print(\"self.policy_trainer.exploration_probability :\", self.policy_trainer.exploration_probability)\n",
        "\n",
        "  def newGame(self):\n",
        "    \"\"\" resets state for the next game that player has to play \"\"\"\n",
        "    self.policy_trainer.newGame()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiFkHSUvsMPb"
      },
      "source": [
        "# PolicyTrainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLEV9tTvYLQZ"
      },
      "source": [
        "class PolicyTrainer:\n",
        "  \"\"\"\n",
        "      exploration_probability (float) epsilon greedy value\n",
        "      learning_rate (float)\n",
        "      discount_factor (float)\n",
        "      Q (dict) - {state_hash : array of grid positions}\n",
        "  \"\"\"\n",
        "  def __init__(self, symbol, exploration_probability, learning_rate = 0.9, discount_factor = 0.95, grid_size = 3):\n",
        "    self.Q = {} #beware!!! when using the initialisation value a {} in the function parameter, then python allocates the same one to both policies\n",
        "    self.learning_rate = learning_rate\n",
        "    self.discount_factor = discount_factor\n",
        "    self.exploration_probability = exploration_probability\n",
        "    self.grid_size = grid_size\n",
        "    self.symbol = symbol\n",
        "    self.move_history = [] #type: List[(state_hash, action)] keeps a history of all moves done to back-propagate the reward\n",
        "    return\n",
        "\n",
        "  def getActionIndex(self, current_action):\n",
        "    \"\"\" Returns index in the action 1D array position where the q value should be put \"\"\"\n",
        "    idx = self.grid_size * current_action[0] + current_action[1]\n",
        "    return idx\n",
        "\n",
        "  def initQValues(self, current_state_hash, possible_actions):\n",
        "    \"\"\" Initialises Q values to -2 for illegal positions where X/O are present, and 0 for where position is empty and can put X/O \"\"\"\n",
        "    if current_state_hash not in self.Q:\n",
        "      self.Q[current_state_hash] = np.full(self.grid_size * self.grid_size, -2.0)\n",
        "      q_values = self.Q[current_state_hash]\n",
        "      for action in possible_actions:\n",
        "        idx = self.getActionIndex(action)\n",
        "        q_values[idx] = 0\n",
        "\n",
        "  def rewardFunction(self, game_board):\n",
        "    \"\"\" when the chosen action is performed on a state, then we get a new state\n",
        "    and associated reward from this transition is computed here\n",
        "    if next state is winner then reward is 1 else -1 if loses to the opponent or 0.5 if draw \"\"\"\n",
        "    \n",
        "    reward = 0 \n",
        "    winner = game_board.checkWinner() # Returns 0 if there is no winner\n",
        "    if winner != 0: #winner is when it's 1 or -1\n",
        "      if winner == self.symbol:\n",
        "          reward = 1\n",
        "      else:\n",
        "          reward = -1\n",
        "    elif game_board.isBoardFull(): #if the game finishes but there is no winner, then reward is 0.5 for draw\n",
        "      reward = 0.5\n",
        "\n",
        "    return reward\n",
        "\n",
        "  def chooseAction(self, state_hash, possible_actions):\n",
        "    \"\"\" choose action per epsilon greedy explore/exploit policy \"\"\"\n",
        "    # #Explore\n",
        "     #doing exploration here causes large fluctuations in the value functions in the topmost level 0 0 0 0 0 0 0 0 0...\n",
        "     #as the max number of states between start of board to a full board is 8 so any back propogated value with \n",
        "     #discount of >.9 will fluctuate the values at the top \n",
        "    if random.random() < self.exploration_probability: \n",
        "      action = self.chooseRandomAction(possible_actions)\n",
        "    else:\n",
        "      #Exploit\n",
        "      action = self.chooseBestAction(state_hash, possible_actions)\n",
        "    return action\n",
        "\n",
        "  def chooseRandomAction(self, possible_actions):\n",
        "    \"\"\" choose random action from list of possible actions in a state \"\"\"\n",
        "    random_idx = np.random.choice(possible_actions.shape[0]) #range from count of possible actions eg 6 actions then [0...5]\n",
        "    action_pos = possible_actions[random_idx]\n",
        "    return action_pos\n",
        "\n",
        "  def chooseBestAction(self, current_state_hash, possible_actions):\n",
        "    \"\"\" Get best action given a set of possible actions in a given state \"\"\"\n",
        "    # Pick a random action at first\n",
        "    random_idx = np.random.choice(possible_actions.shape[0])\n",
        "    best_action = possible_actions[random_idx]\n",
        "\n",
        "    # Find action that given largest Q in given state\n",
        "    maxQ = 0 \n",
        "    for action in possible_actions:\n",
        "      idx = self.getActionIndex(action)\n",
        "      tmpQ = self.Q[current_state_hash][idx]\n",
        "      if maxQ < tmpQ:\n",
        "        maxQ = tmpQ\n",
        "        best_action = action\n",
        "\n",
        "    return best_action\n",
        "\n",
        "  def performAction(self, current_state, next_action):\n",
        "    \"\"\" make the move on the board but defer calculating the Q till the end as \n",
        "        only at the end will we know the reward and so the true value of these steps\n",
        "    \"\"\"\n",
        "    self.move_history.append((current_state.getStateHash(), next_action))\n",
        "    next_state = current_state.performAction(next_action, self.symbol)\n",
        "\n",
        "  def applyFinalResult(self, game_board):\n",
        "    \"\"\" Implements Q-learning iterative algorithm \n",
        "    back propagate the reward to the states that the player sees/moves. \n",
        "    Back propagation is required as the player can judge the value of the moves only at the end\n",
        "    also since the other player is making every alternate view, calculating q value when he makes the move is not possible for the first player\n",
        "    as the policy of the other player makes that move, and q-learning calculation done at that time only applies to the other player\n",
        "    and by the time the first player sees the move it does not remember the last move to adjust the q_value of that move wrt the other players move\n",
        "    easy to do this at the end of all moves\n",
        "    \"\"\"\n",
        "    reward = self.rewardFunction(game_board)\n",
        "\n",
        "    self.move_history.reverse()\n",
        "\n",
        "    next_maxQ = None #type: float\n",
        "    \n",
        "    for (state_hash, action) in self.move_history:\n",
        "      q_values = self.Q[state_hash]\n",
        "      idx = self.getActionIndex(action)\n",
        "      if next_maxQ == None: #first time in the loop\n",
        "        q_values[idx] = reward #as this is action that led to terminal state (win lose or draw), it gets the final reward TODO: should I add the reward\n",
        "      else:\n",
        "        q_values[idx] = q_values[idx] * (1 - self.learning_rate) + self.learning_rate * (0 + self.discount_factor * next_maxQ) #intermediate rewards is zero\n",
        "\n",
        "      next_maxQ = max(q_values)\n",
        "\n",
        "    #print out how the topmost board state Q values change to see when they settle down\n",
        "    # if self.symbol == 1:\n",
        "    #   print(\"printing for symbol X\")\n",
        "    #   for (state_hash, _) in self.move_history:\n",
        "    #     q_values = self.Q[state_hash]\n",
        "    #     print(state_hash, \" \", q_values)\n",
        "    #   print(\"final result :\", self.symbol, reward, self.move_history)\n",
        "\n",
        "\n",
        "  def newGame(self):\n",
        "    \"\"\" Called when a new game is about to start. Reset our internal game state. \"\"\"\n",
        "    self.move_history = []\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHcSV1SNQcsM"
      },
      "source": [
        "# Q-Learning : Simulation Logic for Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZIXiKY_RtYf"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import random"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KTrErYyo0mE"
      },
      "source": [
        "def simulate(playerX, playerO, num_sets = 100, num_games_per_set = 100):\n",
        "\n",
        "  playerX_wins = []\n",
        "  playerO_wins = []\n",
        "  draws = []\n",
        "  count = []    \n",
        "\n",
        "  for i in tqdm(range(num_sets)):\n",
        "    playerX_win, playerO_win, draw = playSet(playerX, playerO, num_games_per_set)\n",
        "    playerX.epsilonDecayPerIterations(i)\n",
        "    playerO.epsilonDecayPerIterations(i)\n",
        "    playerX_wins.append(playerX_win*100.0/num_sets)\n",
        "    playerO_wins.append(playerO_win*100.0/num_sets)\n",
        "    draws.append(draw*100.0/num_sets)\n",
        "    count.append(i*num_sets)\n",
        "\n",
        "  print(\"playerX_wins :\", playerX_wins)\n",
        "  print(\"playerO_wins :\", playerO_wins)\n",
        "  print(\"draws :\", draws)\n",
        "\n",
        "\n",
        "def playSet(playerX, playerO, num_games_per_set):\n",
        "  # Counters for wins of each agent and total number of games\n",
        "  nbr_wins_playerX = 0\n",
        "  nbr_wins_playerO = 0\n",
        "  nbr_draw = 0\n",
        "\n",
        "  for j in range(num_games_per_set):\n",
        "    # Construct game board and reset player states\n",
        "    print(\"\\nNew Game\")\n",
        "    game = Board()\n",
        "    playGame(game, playerX, playerO)\n",
        "\n",
        "    # Check if there is a winner\n",
        "    winner = game.checkWinner() # Returns 0 if there is no winner\n",
        "    if winner == playerX.symbol:\n",
        "      print(\"X wins\")\n",
        "      nbr_wins_playerX += 1\n",
        "    elif winner == playerO.symbol:\n",
        "      print(\"O wins\")\n",
        "      nbr_wins_playerO += 1\n",
        "    else:\n",
        "      print(\"Draw game\")\n",
        "      nbr_draw += 1\n",
        "\n",
        "  return nbr_wins_playerX, nbr_wins_playerO, nbr_draw\n",
        "\n",
        "\n",
        "def playGame(game_board, playerX, playerO):\n",
        "  playerX.newGame()\n",
        "  playerO.newGame()\n",
        "\n",
        "  # Pick current player\n",
        "  current_player = playerX\n",
        "\n",
        "  # play full games in each iteration\n",
        "  while not game_board.checkGameEnded():\n",
        "    possible_actions = game_board.getAvailablePos()\n",
        "    state_hash = game_board.getStateHash()\n",
        "\n",
        "    current_player.performActionPerPolicy(state_hash, possible_actions, game_board)\n",
        "    # print(current_player.symbol, \" after move\")\n",
        "    # game_board.showBoard()\n",
        "    \n",
        "    # Swap player\n",
        "    if current_player == playerX:\n",
        "        current_player = playerO\n",
        "    else:\n",
        "        current_player = playerX\n",
        "\n",
        "  playerX.applyFinalResult(game_board)\n",
        "  playerO.applyFinalResult(game_board)\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKnSaWTQaJcy"
      },
      "source": [
        "# Q-Learning : Self Play"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRyIOe_osfEx"
      },
      "source": [
        "# Epsilon-greedy \n",
        "exploration_probability = 1.0 \n",
        "\n",
        "# Initialise players\n",
        "playerX = Agent(Board.playerX, exploration_probability)\n",
        "playerO = Agent(Board.playerO, exploration_probability)\n",
        "\n",
        "simulate(playerX, playerO, 200, 50)\n",
        "# to Test how the policy and backpropagation logic is working for just one game \n",
        "# simulate(playerX, playerO, 1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q35e6YaguXEE"
      },
      "source": [
        "# Player X's policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u07cS-U5muRf"
      },
      "source": [
        "float_formatter = \"{:.2f}\".format\n",
        "np.set_printoptions(formatter={'float_kind':float_formatter})"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNgCwoz6SCQW"
      },
      "source": [
        "playerX.policy_trainer.Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFujW1YTuiYc"
      },
      "source": [
        "# Player O's policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHtbnVw6bdHy"
      },
      "source": [
        "playerO.policy_trainer.Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnLeAGYcaT13"
      },
      "source": [
        "### How the episodes settle the Q-values"
      ]
    }
  ]
}