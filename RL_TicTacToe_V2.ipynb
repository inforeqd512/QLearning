{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_TicTacToe_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPa0+nkID3621q2yrIYcrxC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inforeqd512/QLearning/blob/main/RL_TicTacToe_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09zQDJ8boz15"
      },
      "source": [
        "import numpy as np\n",
        "import pylab as plt"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhMbEyXuXjS7"
      },
      "source": [
        "# Game\n",
        "\n",
        "Every simulation starts with a new Game _Board_. \n",
        "\n",
        "Each iteration of the simulation ends when the board is full or when any of the players wins.\n",
        "\n",
        "Every Player _Agent_ is setup with their _Policy Trainer_ and follows espilon greedy policy to learn best actions over several iterations of the simulation.\n",
        "\n",
        "**measure of success** : The Players become good at playing when the 'Draw' percentages are high.. ie. the first player to go learns to play in the middle or lears to play to Draw\n",
        "\n",
        "It takes approximately 5000 iterations to get to optimal q-values. At that time both players learn the game well enough to win approx 50% of the time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs_GTXqantDk"
      },
      "source": [
        "### Actors in the Game"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaQuOM1Mnu90"
      },
      "source": [
        "\n",
        "_Board_ is the environment. _State_ is the position of the board after a move from any player. Board also provides the possible set of actions. These are set of open positions on the board on which the player can put it's symbol. \n",
        "\n",
        "_Agent_ is the player playing the game. \n",
        "\n",
        "- Player playing $X$ will be denoted by symbol $1$\n",
        "- Player playing $O$ will be denoted by symbol $-1$\n",
        "\n",
        "Every _Agent_ has its own _Policy Trainer_ that learns the best action it should do in the _Board_ environment. \n",
        "\n",
        "The _Policy_Trainer_ follows is the epsilon greedy random policy to choose the next action. Based on this policy it learns the _Quality Action or Q-table_ for each player. Typically the Q-values of Player X are different to those of Player O. \n",
        "\n",
        "The _Policy_Trainer_ also needs to update the Q-values after performing the action and seeing the reward, so the _Agent_ delegates the choosing of the action and the performing of the action to it. \n",
        "\n",
        "\n",
        "_Action_ to be performed is chosen by the agent on the current state of the board. It is the next available position on the board that the agent can choose from. \n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKTADaJ5V2Wh"
      },
      "source": [
        "### Exploration and Exploitation\n",
        "\n",
        "Initially we explore more as we learn about the possible moves that can be made. Over multiple iterations we start exploiting what we've learnt so we decay the exploration factor epsilon. \n",
        "\n",
        "**Decaying epsilon-greedy**: Does the same as epsilon-greedy, however, the epsilon value starts out near 1, and decays over time according to γ to power of x where x represents the iteration the agent is in.  For γ 0.99 is used \n",
        "\n",
        "<sup>Source: [The Effect of the Exploration\n",
        "Strategy on an Agent’s Performance](https://theses.ubn.ru.nl/bitstream/handle/123456789/5216/Nieuwdorp,%20T._BSc_Thesis_2017.pdf?sequence=1) from Thijs Nieuwdorp</sup>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWBNNhH_Veiz"
      },
      "source": [
        "### Quality of Action \n",
        "\n",
        "**Q (dict)** - \n",
        "\n",
        "- key = state hash, \n",
        "- value = is an array representing 9 spaces in the tic tac toe grid,\n",
        "\n",
        "Starting from top row of grid, from left to right then next row from left to right etc, based on the grid position sent as input action, the array index is computed\n",
        "      \n",
        "eg \n",
        "- for 0,1 grid position = 0 * 3 (number of rows and cols in grid) + 1 == 1 so the index of 1.. ie at position 2 in the array\n",
        "- for 1,1 grid position = 1 * 3 + 1 == 4 so 0,1,2,3,4th position in the array \n",
        "\n",
        "beware!!! when using the default initialisation of parameter as {} then python allocates the same memory to both instances of PolicyTrainer. As a result both policies showed the same scores. Also showed win for 1 when should show only for -1.. The best way to find this happening is to initialise the game state to a board where only one player can win, but you'll see that both players get a non-zero Q-value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO0tWLLZnHm1"
      },
      "source": [
        "# Board"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtDnMMJ_nHP2"
      },
      "source": [
        "class Board:\n",
        "  \"\"\" Class that represents the game board of Tic Tac Toe \"\"\"\n",
        "\n",
        "  playerX = 1\n",
        "  playerO = -1\n",
        "\n",
        "  def __init__(self, rows=3, cols=3):\n",
        "    self.rows = rows\n",
        "    self.cols = cols \n",
        "    self.state = np.zeros((self.rows, self.cols), dtype=np.int8)\n",
        "    # self.state = np.array(\n",
        "    #               [[0,  0, -1],\n",
        "    #                [0,  1,  1],\n",
        "    #                [-1, 1, -1]])\n",
        "    \n",
        "  def showBoard(self):\n",
        "    # p1: x  p2: o\n",
        "    for i in range(0, self.rows):\n",
        "        print('-------------')\n",
        "        out = '| '\n",
        "        for j in range(0, self.cols):\n",
        "            if self.state[i, j] == 1:\n",
        "                token = 'x'\n",
        "            if self.state[i, j] == -1:\n",
        "                token = 'o'\n",
        "            if self.state[i, j] == 0:\n",
        "                token = ' '\n",
        "            out += token + ' | '\n",
        "        print(out)\n",
        "    print('-------------')\n",
        "\n",
        "\n",
        "  def checkWinner(self):\n",
        "    \"\"\"  return winner symbol, if one exists. 0 if no winner\"\"\"\n",
        "\n",
        "    symbols = np.unique(self.state) #unique values , 0, 1, -1\n",
        "    symbols = symbols[np.nonzero(symbols)] #remove 0's\n",
        "    winning_symbol = 0 #no winner yet\n",
        "\n",
        "    for symbol in symbols:\n",
        "      #check rows\n",
        "      row = np.any(np.all(self.state == symbol, axis=1))\n",
        "\n",
        "      #check cols\n",
        "      col = np.any(np.all(self.state == symbol, axis=0))\n",
        "\n",
        "      #check diagonals\n",
        "      diag1 = np.array([self.state[0,0], self.state[1,1], self.state[2,2]])\n",
        "      diag1 = np.all(diag1 == symbol)\n",
        "\n",
        "      diag2 = np.array([self.state[2,0], self.state[1,1], self.state[0,2]])\n",
        "      diag2 = np.all(diag2 == symbol)\n",
        "\n",
        "      # Check if state has winner and return winner in that case\n",
        "      if row or col or diag1 or diag2:\n",
        "        winning_symbol = symbol\n",
        "        break\n",
        "  \n",
        "    return winning_symbol\n",
        "\n",
        "  def getAvailablePos(self):\n",
        "    \"\"\"  Get state positions that have no value ie zeros \"\"\"\n",
        "    return np.argwhere(self.state == 0)\n",
        "\n",
        "  def isBoardFull(self):\n",
        "    return (len(self.getAvailablePos()) == 0)\n",
        "\n",
        "  def checkGameEnded(self):\n",
        "    \"\"\" Check if game has ended by observing if there any possible moves left\n",
        "    or there is a winner \"\"\"\n",
        "    ended = (self.isBoardFull()) or (self.checkWinner() != 0)\n",
        "    return ended\n",
        "\n",
        "  def setPosition(self, x, y, symbol):\n",
        "    \"\"\"  Set state at position (x,y) with symbol \"\"\"\n",
        "    self.state[x,y] = symbol\n",
        "\n",
        "  def getStateHash(self):\n",
        "    \"\"\"  Get hash key of state \"\"\"\n",
        "    boardHash = str(self.state.reshape(self.rows * self.cols))\n",
        "    return boardHash\n",
        "  \n",
        "  def performAction(self, action_to_perform, symbol):\n",
        "    \"\"\"  Perform the action on current board \"\"\"\n",
        "    self.setPosition(action_to_perform[0], action_to_perform[1], symbol)\n",
        "    return self\n",
        "      "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSWPabSDToIi"
      },
      "source": [
        "class Agent:\n",
        "  \"\"\" Class that represents the player \n",
        "      symbol is 1 for 'X' or -1 for 'O' \n",
        "      policy_trainer is initialised with epsilon greedy set to exploration_probability which is reduced overtime\"\"\"\n",
        "\n",
        "  def __init__(self, symbol, exploration_probability):\n",
        "    self.symbol = symbol\n",
        "    self.policy_trainer = PolicyTrainer(exploration_probability=exploration_probability, symbol=symbol)\n",
        "    return\n",
        "\n",
        "  def performActionPerPolicy(self, state_hash, possible_actions, current_state):\n",
        "    \"\"\" per the explore and exploitation with current epsilon \"\"\"\n",
        "    next_action = self.policy_trainer.chooseAction(state_hash, possible_actions)\n",
        "    self.policy_trainer.performAction(current_state, next_action)\n",
        "\n",
        "  def applyFinalResult(self, game_board):\n",
        "    \"\"\"\n",
        "      Gets called after the game has finished. Will update the current Q function based on the game outcome.\n",
        "    \"\"\"\n",
        "    self.policy_trainer.applyFinalResult(game_board)\n",
        "\n",
        "  def epsilonDecayPerIterations(self, num_iterations):\n",
        "    \"\"\" gradually reduces probabilities per iteration but not completely eliminate it\"\"\"\n",
        "    # Reduce probability to explore during training\n",
        "    # Do not remove completely \n",
        "    \n",
        "    # Decaying epsilon-greedy: Does the same as epsilon-greedy, however, the epsilon value starts out near 1,\n",
        "    # and decays over time according to γ to power of x where x represents the iteration the agent is in. \n",
        "    # For γ 0.99 is used per https://theses.ubn.ru.nl/bitstream/handle/123456789/5216/Nieuwdorp,%20T._BSc_Thesis_2017.pdf?sequence=1\n",
        "        \n",
        "    if self.policy_trainer.exploration_probability > 0.2:\n",
        "      decay = 0.99 ** num_iterations\n",
        "      self.policy_trainer.exploration_probability = decay\n",
        "      print(\"self.policy_trainer.exploration_probability :\", self.policy_trainer.exploration_probability)\n",
        "\n",
        "  def updateScore(self):\n",
        "    \"\"\" measures how much is the q-values changing over iterations till finally they settle \"\"\"\n",
        "    self.policy_trainer.updateScore()\n",
        "\n",
        "  def newGame(self):\n",
        "    self.policy_trainer.newGame()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLEV9tTvYLQZ"
      },
      "source": [
        "class PolicyTrainer:\n",
        "  \"\"\"\n",
        "      exploration_probability (float) epsilon greedy value\n",
        "      learning_rate (float)\n",
        "      discount_factor (float)\n",
        "      Q (dict) - {state_hash : array of grid positions}\n",
        "  \"\"\"\n",
        "  def __init__(self, symbol, exploration_probability, learning_rate = 0.9, discount_factor = 0.95, grid_size = 3):\n",
        "    self.Q = {} #beware!!! when using the initiatlisation value a {} then python allocates the same one to both policies\n",
        "    self.learning_rate = learning_rate\n",
        "    self.discount_factor = discount_factor\n",
        "    print(\"initialise exploration_probability :\", exploration_probability)\n",
        "    self.exploration_probability = exploration_probability\n",
        "    self.grid_size = grid_size\n",
        "    self.scores = []\n",
        "    self.symbol = symbol\n",
        "    self.num_times_states_seen = {}\n",
        "    self.move_history = [] #type: List[(state_hash, action)] keeps a history of all moves done to back propagate the reward\n",
        "    return\n",
        "\n",
        "  def getActionIndex(self, current_action):\n",
        "    \"\"\" Returns index in the action array position where the q value should be put \"\"\"\n",
        "    idx = self.grid_size * current_action[0] + current_action[1]\n",
        "    return idx\n",
        "\n",
        "  def getQValues(self, current_state_hash) -> np.ndarray:\n",
        "    if current_state_hash not in self.Q:\n",
        "      self.Q[current_state_hash] = np.zeros(self.grid_size * self.grid_size)\n",
        "    q_values = self.Q[current_state_hash]\n",
        "    return q_values\n",
        "\n",
        "  def getValueQ(self, current_state_hash, current_action) -> float:\n",
        "    \"\"\" Get the quality value of a given action in a given state,\n",
        "            returns 0 if the state-action pair has not been seen before and in this case it adds that state into the Q table\n",
        "            Input is state hash key\n",
        "            and action as an array of position where the symbol should be put ie [0,1] for top row middle square\"\"\"\n",
        "    idx = self.getActionIndex(current_action)\n",
        "    q_value = 0.0\n",
        "    if current_state_hash in self.Q:\n",
        "      q_value = self.Q[current_state_hash][idx]\n",
        "    else:\n",
        "      self.setValueQ(current_state_hash, current_action, q_value)\n",
        "      \n",
        "    return q_value\n",
        "\n",
        "  def setValueQ(self, current_state_hash, current_action, value):\n",
        "    \"\"\" Set value in Q \n",
        "    if this is the first time the state is seen, then the value is the array representing the grid\"\"\"\n",
        "    idx = self.getActionIndex(current_action)\n",
        "    if current_state_hash in self.Q:\n",
        "      self.Q[current_state_hash][idx] = value\n",
        "    else:\n",
        "      self.Q[current_state_hash] = np.zeros(self.grid_size * self.grid_size)\n",
        "      self.Q[current_state_hash][idx] = value\n",
        "    return\n",
        "\n",
        "  def rewardFunction(self, game_board):\n",
        "    \"\"\" when the chosen action is performed on a state, then we get a new state\n",
        "    and associated reward from this transition is computed here\n",
        "    if next state is winner then reward is 1 else -1 if loses to the opponent or 0.5 if draw \"\"\"\n",
        "    \n",
        "    reward = 0 \n",
        "    winner = game_board.checkWinner() # Returns 0 if there is no winner\n",
        "    if winner != 0: #winner is when it's 1 or -1\n",
        "      if winner == self.symbol:\n",
        "          reward = 1\n",
        "      else:\n",
        "          reward = -1\n",
        "    elif game_board.isBoardFull(): #if the game finishes but there is no winner, then reward is 0.5 for draw\n",
        "      reward = 0.5\n",
        "\n",
        "    return reward\n",
        "\n",
        "  def chooseAction(self, state_hash, possible_actions):\n",
        "    \"\"\" choose action per epsilon greedy explore/exploit policy \"\"\"\n",
        "    #Explore\n",
        "    if random.random() < self.exploration_probability:\n",
        "      action = self.chooseRandomAction(possible_actions)\n",
        "    else:\n",
        "      #Exploit\n",
        "      action = self.chooseBestAction(state_hash, possible_actions)\n",
        "\n",
        "    if state_hash == '[0 0 0 0 0 0 0 0 0]' and self.symbol == 1:\n",
        "      print(state_hash, \" \", action)\n",
        "    return action\n",
        "\n",
        "  def chooseRandomAction(self, possible_actions):\n",
        "    \"\"\" choose random action from list of possible actions in a state \"\"\"\n",
        "    random_idx = np.random.choice(possible_actions.shape[0]) #range from count of possible actions eg 6 actions then [0...5]\n",
        "    action_pos = possible_actions[random_idx]\n",
        "    return action_pos\n",
        "\n",
        "  def chooseBestAction(self, current_state_hash, possible_actions):\n",
        "    \"\"\" Get best action given a set of possible actions in a given state \"\"\"\n",
        "    # Pick a random action at first\n",
        "    random_idx = np.random.choice(possible_actions.shape[0])\n",
        "    best_action = possible_actions[random_idx]\n",
        "\n",
        "    # Find action that given largest Q in given state\n",
        "    maxQ = 0 \n",
        "    for action in possible_actions:\n",
        "      tmpQ = self.getValueQ(current_state_hash, action)\n",
        "      if maxQ < tmpQ:\n",
        "        maxQ = tmpQ\n",
        "        best_action = action\n",
        "\n",
        "    return best_action\n",
        "\n",
        "  def performAction(self, current_state, next_action):\n",
        "    \"\"\" make the move on the board but defer calculating the Q till the end as \n",
        "        only at the end will we know the reward and so the true value of these steps\n",
        "    \"\"\"\n",
        "    self.move_history.append((current_state.getStateHash(), next_action))\n",
        "    next_state = current_state.performAction(next_action, self.symbol)\n",
        "\n",
        "  def applyFinalResult(self, game_board):\n",
        "    \"\"\" Implements Q-learning iterative algorithm \n",
        "    back propagate the reward to the states that the player sees/moves. \n",
        "    Back propagation is required as the player can judge the value of the moves only at the end\n",
        "    also since the other player is making every alternate view, calculating q value when he makes the move is not possible for the first player\n",
        "    as the policy of the other player makes that move, and q-learning calculation done at that time only applies to the other player\n",
        "    and by the time the first player sees the move it does not remember the last move to adjust the q_value of that move wrt the other players move\n",
        "    easy to do this at the end of all moves\n",
        "    \"\"\"\n",
        "    reward = self.rewardFunction(game_board)\n",
        "\n",
        "    self.move_history.reverse()\n",
        "\n",
        "    next_maxQ = None #type: float\n",
        "    \n",
        "    for (state_hash, action) in self.move_history:\n",
        "      q_values = self.getQValues(state_hash)\n",
        "      idx = self.getActionIndex(action)\n",
        "      if next_maxQ == None: #first time in the loop\n",
        "        q_values[idx] = reward #as this is action that led to terminal state (win lose or draw), it gets the final reward TODO: should I add the reward\n",
        "      else:\n",
        "        q_values[idx] = q_values[idx] * (1 - self.learning_rate) + self.learning_rate * (0 + self.discount_factor * next_maxQ) #intermediate rewards is zero\n",
        "      next_maxQ = max(q_values)\n",
        "\n",
        "    if self.symbol == 1:\n",
        "      for (state_hash, _) in self.move_history:\n",
        "        q_values = self.getQValues(state_hash)\n",
        "        print(state_hash, \" \", q_values)\n",
        "      print(\"final result :\", self.symbol, reward, self.move_history)\n",
        "\n",
        "\n",
        "  def newGame(self):\n",
        "    \"\"\"\n",
        "    Called when a new game is about to start. Reset our internal game state.\n",
        "    \"\"\"\n",
        "    self.move_history = []\n",
        "\n",
        "  def updateScore(self):\n",
        "    #compute score\n",
        "    score = 0\n",
        "    list1 = list(self.Q.values())\n",
        "    score = np.sum(list1)\n",
        "    self.scores.append(score)\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHcSV1SNQcsM"
      },
      "source": [
        "# Q-Learning : Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZIXiKY_RtYf"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import random"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KTrErYyo0mE"
      },
      "source": [
        "def simulate(playerX, playerO, num_sets = 100, num_games_per_set = 100):\n",
        "\n",
        "  playerX_wins = []\n",
        "  playerO_wins = []\n",
        "  draws = []\n",
        "  count = []    \n",
        "\n",
        "  for i in tqdm(range(num_sets)):\n",
        "    playerX_win, playerO_win, draw = playSet(playerX, playerO, num_games_per_set)\n",
        "    playerX.epsilonDecayPerIterations(i)\n",
        "    playerO.epsilonDecayPerIterations(i)\n",
        "    playerX_wins.append(playerX_win*100.0/num_sets)\n",
        "    playerO_wins.append(playerO_win*100.0/num_sets)\n",
        "    draws.append(draw*100.0/num_sets)\n",
        "    count.append(i*num_sets)\n",
        "\n",
        "  print(\"playerX_wins :\", playerX_wins)\n",
        "  print(\"playerO_wins :\", playerO_wins)\n",
        "  print(\"draws :\", draws)\n",
        "\n",
        "\n",
        "def playSet(playerX, playerO, num_games_per_set):\n",
        "  # Counters for wins of each agent and total number of games\n",
        "  nbr_wins_playerX = 0\n",
        "  nbr_wins_playerO = 0\n",
        "  nbr_draw = 0\n",
        "\n",
        "  for j in range(num_games_per_set):\n",
        "    # Construct game board and reset player states\n",
        "    print(\"\\nNew Game\")\n",
        "    game = Board()\n",
        "    playGame(game, playerX, playerO)\n",
        "\n",
        "    # Check if there is a winner\n",
        "    winner = game.checkWinner() # Returns 0 if there is no winner\n",
        "    if winner == playerX.symbol:\n",
        "      print(\"X wins\")\n",
        "      nbr_wins_playerX += 1\n",
        "    elif winner == playerO.symbol:\n",
        "      print(\"O wins\")\n",
        "      nbr_wins_playerO += 1\n",
        "    else:\n",
        "      print(\"Draw game\")\n",
        "      nbr_draw += 1\n",
        "\n",
        "  return nbr_wins_playerX, nbr_wins_playerO, nbr_draw\n",
        "\n",
        "\n",
        "def playGame(game_board, playerX, playerO):\n",
        "  playerX.newGame()\n",
        "  playerO.newGame()\n",
        "\n",
        "  # Pick current player\n",
        "  current_player = playerX\n",
        "\n",
        "  # play full games in each iteration\n",
        "  while not game_board.checkGameEnded():\n",
        "    possible_actions = game_board.getAvailablePos()\n",
        "    state_hash = game_board.getStateHash()\n",
        "\n",
        "    current_player.performActionPerPolicy(state_hash, possible_actions, game_board)\n",
        "    # print(current_player.symbol, \" after move\")\n",
        "    # game_board.showBoard()\n",
        "    \n",
        "    # Swap player\n",
        "    if current_player == playerX:\n",
        "        current_player = playerO\n",
        "    else:\n",
        "        current_player = playerX\n",
        "\n",
        "  playerX.applyFinalResult(game_board)\n",
        "  playerO.applyFinalResult(game_board)\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKnSaWTQaJcy"
      },
      "source": [
        "# Q-Learning : Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRyIOe_osfEx",
        "outputId": "08f5a1e3-08c7-4e01-efb1-a19fc067c796"
      },
      "source": [
        "# Epsilon-greedy \n",
        "exploration_probability = 1.0 \n",
        "#1.0\n",
        "\n",
        "# Initiatlise players\n",
        "playerX = Agent(Board.playerX, exploration_probability)\n",
        "playerO = Agent(Board.playerO, exploration_probability)\n",
        "\n",
        "# simulate(playerX, playerO, 100, 50)\n",
        "simulate(playerX, playerO, 1, 1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initialise exploration_probability : 1.0\n",
            "initialise exploration_probability : 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 62.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New Game\n",
            "[0 0 0 0 0 0 0 0 0]   [1 0]\n",
            "[-1  1  0  1  0  1 -1  0 -1]   [0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00]\n",
            "[-1  0  0  1  0  1 -1  0  0]   [0.00 0.85 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            "[ 0  0  0  1  0  0 -1  0  0]   [0.00 0.00 0.00 0.00 0.00 0.73 0.00 0.00 0.00]\n",
            "[0 0 0 0 0 0 0 0 0]   [0.00 0.00 0.00 0.63 0.00 0.00 0.00 0.00 0.00]\n",
            "final result : 1 1 [('[-1  1  0  1  0  1 -1  0 -1]', array([1, 1])), ('[-1  0  0  1  0  1 -1  0  0]', array([0, 1])), ('[ 0  0  0  1  0  0 -1  0  0]', array([1, 2])), ('[0 0 0 0 0 0 0 0 0]', array([1, 0]))]\n",
            "X wins\n",
            "self.policy_trainer.exploration_probability : 1.0\n",
            "self.policy_trainer.exploration_probability : 1.0\n",
            "playerX_wins : [100.0]\n",
            "playerO_wins : [0.0]\n",
            "draws : [0.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5ynzHkISBxX"
      },
      "source": [
        "# (playerX, playerO) = simulate(10000)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm8THScbR4cE"
      },
      "source": [
        "# plt.plot(playerX.policy_trainer.scores)\n",
        "# plt.show()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNgCwoz6SCQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc5e8a3-74c5-4602-ceef-94873cd8702e"
      },
      "source": [
        "playerX.policy_trainer.Q"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[ 0  0  0  1  0  0 -1  0  0]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.73, 0.00, 0.00, 0.00]),\n",
              " '[-1  0  0  1  0  1 -1  0  0]': array([0.00, 0.85, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[-1  1  0  1  0  1 -1  0 -1]': array([0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[0 0 0 0 0 0 0 0 0]': array([0.00, 0.00, 0.00, 0.63, 0.00, 0.00, 0.00, 0.00, 0.00])}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHtbnVw6bdHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcb970e2-7e1d-4eff-dca6-c1b454e9d8a7"
      },
      "source": [
        "playerO.policy_trainer.Q"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[ 0  0  0  1  0  1 -1  0  0]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[-1  1  0  1  0  1 -1  0  0]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, -1.00]),\n",
              " '[0 0 0 1 0 0 0 0 0]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00])}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnLeAGYcaT13"
      },
      "source": [
        "### How the episodes settle the Q-values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6Uks9AbHKG2"
      },
      "source": [
        "# plt.plot(playerX.policy_trainer.scores)\n",
        "# plt.show()"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eApg9sDu1si-"
      },
      "source": [
        "# plt.plot(playerO.policy_trainer.scores)\n",
        "# plt.show()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajuzJOhcanUw"
      },
      "source": [
        "### Trained Q-values for Player X\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u07cS-U5muRf"
      },
      "source": [
        "float_formatter = \"{:.2f}\".format\n",
        "np.set_printoptions(formatter={'float_kind':float_formatter})"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvFj0ROANJTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a222bcb-2749-4c14-aefa-c6fea0d7ef33"
      },
      "source": [
        "playerX.policy_trainer.Q"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[ 0  0  0  1  0  0 -1  0  0]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.73, 0.00, 0.00, 0.00]),\n",
              " '[-1  0  0  1  0  1 -1  0  0]': array([0.00, 0.85, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[-1  1  0  1  0  1 -1  0 -1]': array([0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[0 0 0 0 0 0 0 0 0]': array([0.00, 0.00, 0.00, 0.63, 0.00, 0.00, 0.00, 0.00, 0.00])}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8PJK7mKa4Yq"
      },
      "source": [
        "### Trained Q-values for Player O"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RyZifhaZ2MG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673483c1-978a-4e3e-d828-1d2b4f6544b5"
      },
      "source": [
        "playerO.policy_trainer.Q"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[ 0  0  0  1  0  1 -1  0  0]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[-1  1  0  1  0  1 -1  0  0]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, -1.00]),\n",
              " '[0 0 0 1 0 0 0 0 0]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00])}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ]
}