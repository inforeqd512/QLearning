{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_TicTacToe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxzxILIeDQuEzHfNkMUZxY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inforeqd512/QLearning/blob/main/RL_TicTacToe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09zQDJ8boz15"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs_GTXqantDk"
      },
      "source": [
        "# Game"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaQuOM1Mnu90"
      },
      "source": [
        "On the Board\n",
        "\n",
        "Player playing $X$ will be denoted by 1\n",
        "\n",
        "Player playing $O$ will be denoted by -1\n",
        "\n",
        "Board in the environment. State is the position of the board after a move from any player. Board also provides the possible set of actions\n",
        "\n",
        "Action to be performed is chosen by the agent on the current state of the board"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO0tWLLZnHm1"
      },
      "source": [
        "# Board"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtDnMMJ_nHP2"
      },
      "source": [
        "class Board:\n",
        "  \"\"\" Class that represents the game board of Tic Tac Toe \"\"\"\n",
        "\n",
        "  playerX = 1\n",
        "  playerO = -1\n",
        "\n",
        "  def __init__(self, rows=3, cols=3):\n",
        "    self.rows = rows\n",
        "    self.cols = cols \n",
        "    self.resetGame()\n",
        "\n",
        "  def resetGame(self):\n",
        "    self.state = np.zeros((self.rows, self.cols), dtype=np.int8)\n",
        "\n",
        "  def checkWinner(self):\n",
        "    \"\"\"  return winner symbol, if one exists \"\"\"\n",
        "\n",
        "    symbols = np.unique(self.state) #unique values , 0, 1, -1\n",
        "    symbols = symbols[np.nonzero(symbols)] #remove 0's\n",
        "    winning_symbol = 0 #no winner yet\n",
        "\n",
        "    for symbol in symbols:\n",
        "      #check rows\n",
        "      row = np.any(np.all(self.state == symbol, axis=1))\n",
        "\n",
        "      #check cols\n",
        "      col = np.any(np.all(self.state == symbol, axis=0))\n",
        "\n",
        "      #check diagonals\n",
        "      diag1 = np.array([self.state[0,0], self.state[1,1], self.state[2,2]])\n",
        "      diag1 = np.all(diag1 == symbol)\n",
        "\n",
        "      diag2 = np.array([self.state[2,0], self.state[1,1], self.state[0,2]])\n",
        "      diag2 = np.all(diag2 == symbol)\n",
        "\n",
        "      # Check if state has winner and return winner in that case\n",
        "      if row or col or diag1 or diag2:\n",
        "        winning_symbol = symbol\n",
        "        break\n",
        "  \n",
        "    return winning_symbol\n",
        "\n",
        "  def getAvailablePos(self):\n",
        "    \"\"\"  Get state positions that have no value ie zeros \"\"\"\n",
        "    return np.argwhere(self.state == 0)\n",
        "\n",
        "  def checkGameEnded(self):\n",
        "    \"\"\" Check if game has ended by observing if there any possible moves left \"\"\"\n",
        "    return len(self.getAvailablePos()) == 0 \n",
        "\n",
        "  def setPosition(self, x, y, symbol):\n",
        "    \"\"\"  Set state at position (x,y) with symbol \"\"\"\n",
        "    self.state[x,y] = symbol\n",
        "\n",
        "  def getStateHash(self):\n",
        "    \"\"\"  Get hash key of state \"\"\"\n",
        "    return np.array2string(self.state)\n",
        "\n",
        "  def getActionsHash(self):\n",
        "    actions = self.getAvailablePos()\n",
        "    hash_list = []\n",
        "    for action in actions:\n",
        "      hash_list.append(np.array2string(action))\n",
        "    return hash_list\n",
        "  \n",
        "  def getActionHash(self, action):\n",
        "    return np.array2string(action)\n",
        "\n",
        "  def performAction(self, action_to_perform, symbol):\n",
        "    self.setPosition(action_to_perform[0], action_to_perform[1], symbol)\n",
        "    return self\n",
        "      "
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSWPabSDToIi"
      },
      "source": [
        "class Agent:\n",
        "\n",
        "  def __init__(self, symbol, exploration_probability):\n",
        "    self.symbol = symbol\n",
        "    self.policy_trainer = PolicyTrainer(exploration_probability=exploration_probability)\n",
        "    return\n",
        "\n",
        "  def performActionPerPolicy(self, state_hash, actions_hash, possible_actions, current_state):\n",
        "    action = self.policy_trainer.chooseAction(state_hash, actions_hash, possible_actions)\n",
        "    self.policy_trainer.performAction(current_state, action, self.symbol)"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLEV9tTvYLQZ"
      },
      "source": [
        "class PolicyTrainer:\n",
        "  \"\"\"\n",
        "      learning_parameter (float)\n",
        "      discount_factor (float)\n",
        "      Q (dict)\n",
        "  \"\"\"\n",
        "  def __init__(self, exploration_probability, learning_rate = 0.1, discount_factor = 0.9, Q = {}):\n",
        "    self.Q = Q\n",
        "    self.learning_rate = learning_rate\n",
        "    self.discount_factor = discount_factor\n",
        "    self.exploration_probability = exploration_probability\n",
        "    self.currentStateActionKey = None\n",
        "    # self.numIterations()\n",
        "    return\n",
        "\n",
        "\n",
        "  def getStateActionPairKey(self, current_state_hash, current_action_hash):\n",
        "    \"\"\" Returns state-pair hash key, requires separate state and action hash keys first \"\"\"\n",
        "    return current_state_hash + current_action_hash\n",
        "\n",
        "  def getValueQ(self, current_state_hash, current_action_hash):\n",
        "    \"\"\" Get expected reward given an action in a given state,\n",
        "            returns 0 if the state-action pair has not been seen before.\n",
        "            Input is state and action hash key                          \"\"\"\n",
        "    stateActionPairKey = self.getStateActionPairKey(current_state_hash, current_action_hash)\n",
        "    qValue = 0 \n",
        "    if stateActionPairKey in self.Q:\n",
        "      qValue = self.Q[stateActionPairKey]\n",
        "    else:\n",
        "      self.setValueQ(current_state_hash, current_action_hash, qValue)\n",
        "\n",
        "    return qValue\n",
        "\n",
        "  def setValueQ(self, current_state_hash, current_action_hash, value):\n",
        "    \"\"\" Set value in Q \"\"\"\n",
        "    stateActionPairKey = self.getStateActionPairKey(current_state_hash, current_action_hash)\n",
        "    self.Q[stateActionPairKey] = value\n",
        "    return\n",
        "\n",
        "  def rewardFunction(self, next_state, symbol):\n",
        "    \"\"\" Returns positive value actions turns into win, else zero \"\"\"\n",
        "    winner = next_state.checkWinner()\n",
        "    reward = 0 \n",
        "    if winner == symbol:\n",
        "      reward = 1\n",
        "    \n",
        "    return reward\n",
        "\n",
        "\n",
        "  def chooseAction(self, state_hash, actions_hash, possible_actions):\n",
        "    #Explore\n",
        "    if random.random() < self.exploration_probability:\n",
        "      action = self.chooseRandomAction(possible_actions)\n",
        "    else:\n",
        "      #Exploit\n",
        "      action = self.chooseBestAction(state_hash, actions_hash, possible_actions)\n",
        "    return action\n",
        "\n",
        "  def chooseRandomAction(self, possible_actions):\n",
        "    random_idx = np.random.choice(possible_actions.shape[0])\n",
        "    action_pos = possible_actions[random_idx]\n",
        "    return action_pos\n",
        "\n",
        "  def chooseBestAction(self, current_state_hash, actions_hash, possible_actions):\n",
        "    \"\"\" Get best action given a set of possible actions in a given state \"\"\"\n",
        "    # Pick a random action at first\n",
        "    random_idx = np.random.choice(possible_actions.shape[0])\n",
        "    best_action = possible_actions[random_idx]\n",
        "\n",
        "    # Find action that given largest Q in given state\n",
        "    maxQ = 0 \n",
        "    for action_hash, action in zip(actions_hash, possible_actions):\n",
        "      tmpQ = self.getValueQ(current_state_hash, action_hash)\n",
        "      if maxQ < tmpQ:\n",
        "        maxQ = tmpQ\n",
        "        best_action = action\n",
        "\n",
        "    return best_action\n",
        "\n",
        "  def getMaxQ(self, next_state):\n",
        "    actions_hash = next_state.getActionsHash()\n",
        "    state_hash = next_state.getStateHash()\n",
        "\n",
        "    # Find action that given largest Q in given state\n",
        "    maxQ = 0 \n",
        "    for action_hash in actions_hash:\n",
        "      tmpQ = self.getValueQ(state_hash, action_hash)\n",
        "      if maxQ < tmpQ:\n",
        "        maxQ = tmpQ\n",
        "\n",
        "    return maxQ\n",
        "\n",
        "  def performAction(self, current_state, current_action, symbol):\n",
        "    \"\"\" Implements Q-learning iterative algorithm \"\"\"\n",
        "    current_state_hash = current_state.getStateHash()\n",
        "    current_action_hash = current_state.getActionHash(current_action)\n",
        "\n",
        "    # Get current Q Value\n",
        "    currentQ = self.getValueQ(current_state_hash, current_action_hash)\n",
        "\n",
        "    next_state = current_state.performAction(current_action, symbol)\n",
        "\n",
        "    newQ = (1 - self.learning_rate) * currentQ\n",
        "    newQ += self.learning_rate * (self.rewardFunction(next_state, symbol) + self.discount_factor * self.getMaxQ(next_state) - currentQ)\n",
        "\n",
        "    self.setValueQ(current_state_hash, current_action_hash, newQ)\n"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHcSV1SNQcsM"
      },
      "source": [
        "# Q-Learning : Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZIXiKY_RtYf"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import random"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW2Kf4WpnDLu"
      },
      "source": [
        "def simulate(iterations):\n",
        "  \"\"\" iterations (int) \"\"\"\n",
        "\n",
        "  # Construct game board\n",
        "  game = Board()\n",
        "\n",
        " # Epsilon-greedy \n",
        "  exploration_probability = 1.0\n",
        "\n",
        "  # Initiatlise players\n",
        "  playerX = Agent(Board.playerX, exploration_probability)\n",
        "  playerO = Agent(Board.playerO, exploration_probability)\n",
        "\n",
        "  # Counters for wins of each agent and total number of games\n",
        "  nbr_wins_playerX = 0\n",
        "  nbr_wins_playerO = 0\n",
        "  nbr_games = 0\n",
        "\n",
        "  # Pick current player\n",
        "  current_player = playerX\n",
        "\n",
        "  for i in tqdm(range(iterations)):\n",
        "\n",
        "    print(\"\\nGame :\", i)\n",
        "\n",
        "    # Check if games has ended, reset if True\n",
        "    while not game.checkGameEnded():\n",
        "      possible_actions = game.getAvailablePos()\n",
        "      state_hash = game.getStateHash()\n",
        "      actions_hash = game.getActionsHash()\n",
        "      current_player.performActionPerPolicy(state_hash, actions_hash, possible_actions, game)\n",
        "\n",
        "      # Reduce probability to explore during training\n",
        "      # Do not remove completely\n",
        "      if exploration_probability > 0.2:\n",
        "          exploration_probability -= 1/iterations\n",
        "\n",
        "      # Check if there is a winner\n",
        "      winner = game.checkWinner() # Returns 0 if there is no winner\n",
        "      if winner != 0: #if a winner\n",
        "          # Add to count for corresponding winner\n",
        "          if winner == playerX.symbol:\n",
        "              nbr_wins_playerX += 1\n",
        "          else:\n",
        "              nbr_wins_playerO += 1\n",
        "          break\n",
        "\n",
        "      # Swap player\n",
        "      if current_player == playerX:\n",
        "          current_player = playerO\n",
        "      else:\n",
        "          current_player = playerX\n",
        "\n",
        "    nbr_games += 1\n",
        "    if winner == 0:\n",
        "      print(\"\\nDraw\")\n",
        "    else:\n",
        "      print(winner, \"wins\")\n",
        "    print(\"board :\\n\", game.state)\n",
        "    game.resetGame()\n",
        "    \n",
        "  # Print outcome\n",
        "  print(nbr_wins_playerX, nbr_wins_playerO, nbr_games)    \n",
        "  print(\"Win percentage: Agent X {:.2%}, Agent O {:.2%}.\".format(nbr_wins_playerX/nbr_games, nbr_wins_playerO/nbr_games))\n",
        "\n"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upb8dmgV07x-"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5ynzHkISBxX",
        "outputId": "44733558-2bfb-49e0-e1a8-4b8935979daa"
      },
      "source": [
        "simulate(50)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 7/50 [00:00<00:00, 64.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Game : 0\n",
            "\n",
            "Draw\n",
            "board :\n",
            " [[-1  1  1]\n",
            " [ 1  1 -1]\n",
            " [-1 -1  1]]\n",
            "\n",
            "Game : 1\n",
            "\n",
            "Draw\n",
            "board :\n",
            " [[ 1 -1  1]\n",
            " [-1 -1  1]\n",
            " [-1  1 -1]]\n",
            "\n",
            "Game : 2\n",
            "-1 wins\n",
            "board :\n",
            " [[ 1  1  0]\n",
            " [ 0  0  1]\n",
            " [-1 -1 -1]]\n",
            "\n",
            "Game : 3\n",
            "1 wins\n",
            "board :\n",
            " [[ 1  1  1]\n",
            " [-1  0 -1]\n",
            " [ 0  0 -1]]\n",
            "\n",
            "Game : 4\n",
            "1 wins\n",
            "board :\n",
            " [[ 0 -1  1]\n",
            " [-1  1  1]\n",
            " [ 1  0 -1]]\n",
            "\n",
            "Game : 5\n",
            "-1 wins\n",
            "board :\n",
            " [[ 1 -1  1]\n",
            " [-1 -1 -1]\n",
            " [ 0  1  1]]\n",
            "\n",
            "Game : 6\n",
            "-1 wins\n",
            "board :\n",
            " [[-1  1  1]\n",
            " [ 1 -1 -1]\n",
            " [ 1 -1 -1]]\n",
            "\n",
            "Game : 7\n",
            "-1 wins\n",
            "board :\n",
            " [[-1  1  0]\n",
            " [-1  1  1]\n",
            " [-1 -1  0]]\n",
            "\n",
            "Game : 8\n",
            "-1 wins\n",
            "board :\n",
            " [[-1 -1  1]\n",
            " [-1  0  1]\n",
            " [-1  1  0]]\n",
            "\n",
            "Game : 9\n",
            "-1 wins\n",
            "board :\n",
            " [[ 1  0 -1]\n",
            " [ 0  0 -1]\n",
            " [ 1  0 -1]]\n",
            "\n",
            "Game : 10\n",
            "\n",
            "Draw\n",
            "board :\n",
            " [[-1 -1  1]\n",
            " [ 1  1 -1]\n",
            " [-1 -1  1]]\n",
            "\n",
            "Game : 11\n",
            "1 wins\n",
            "board :\n",
            " [[ 0  1 -1]\n",
            " [-1  1  0]\n",
            " [ 0  1  0]]\n",
            "\n",
            "Game : 12\n",
            "-1 wins\n",
            "board :\n",
            " [[ 1 -1  1]\n",
            " [ 0 -1  1]\n",
            " [ 1 -1 -1]]\n",
            "\n",
            "Game : 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 21/50 [00:00<00:00, 63.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 wins\n",
            "board :\n",
            " [[ 1  1  1]\n",
            " [-1  0 -1]\n",
            " [-1  1 -1]]\n",
            "\n",
            "Game : 14\n",
            "\n",
            "Draw\n",
            "board :\n",
            " [[ 1  1 -1]\n",
            " [-1 -1  1]\n",
            " [ 1 -1  1]]\n",
            "\n",
            "Game : 15\n",
            "-1 wins\n",
            "board :\n",
            " [[-1 -1 -1]\n",
            " [ 0 -1  1]\n",
            " [ 1  0  1]]\n",
            "\n",
            "Game : 16\n",
            "-1 wins\n",
            "board :\n",
            " [[-1  0  0]\n",
            " [ 0 -1  1]\n",
            " [ 1  0 -1]]\n",
            "\n",
            "Game : 17\n",
            "1 wins\n",
            "board :\n",
            " [[ 1 -1  1]\n",
            " [-1  1 -1]\n",
            " [-1  0  1]]\n",
            "\n",
            "Game : 18\n",
            "1 wins\n",
            "board :\n",
            " [[-1  1  1]\n",
            " [ 0  0  1]\n",
            " [-1 -1  1]]\n",
            "\n",
            "Game : 19\n",
            "\n",
            "Draw\n",
            "board :\n",
            " [[-1  1 -1]\n",
            " [-1  1  1]\n",
            " [ 1 -1  1]]\n",
            "\n",
            "Game : 20\n",
            "-1 wins\n",
            "board :\n",
            " [[ 1  0 -1]\n",
            " [ 0  1 -1]\n",
            " [ 0  0 -1]]\n",
            "\n",
            "Game : 21\n",
            "-1 wins\n",
            "board :\n",
            " [[ 1 -1 -1]\n",
            " [-1 -1  1]\n",
            " [ 1 -1  1]]\n",
            "\n",
            "Game : 22\n",
            "1 wins\n",
            "board :\n",
            " [[ 1 -1 -1]\n",
            " [ 1  1 -1]\n",
            " [ 0 -1  1]]\n",
            "\n",
            "Game : 23\n",
            "1 wins\n",
            "board :\n",
            " [[ 1 -1  1]\n",
            " [-1 -1  1]\n",
            " [-1  1  1]]\n",
            "\n",
            "Game : 24\n",
            "1 wins\n",
            "board :\n",
            " [[-1  0  1]\n",
            " [-1  1 -1]\n",
            " [ 1  1  0]]\n",
            "\n",
            "Game : 25\n",
            "-1 wins\n",
            "board :\n",
            " [[ 1  1 -1]\n",
            " [ 0  0 -1]\n",
            " [ 0  1 -1]]\n",
            "\n",
            "Game : 26\n",
            "1 wins\n",
            "board :\n",
            " [[ 1 -1  0]\n",
            " [ 1  0  0]\n",
            " [ 1 -1 -1]]\n",
            "\n",
            "Game : 27\n",
            "-1 wins\n",
            "board :\n",
            " [[-1 -1 -1]\n",
            " [ 1  1 -1]\n",
            " [ 0  1  1]]\n",
            "\n",
            "Game : 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 36/50 [00:00<00:00, 60.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 wins\n",
            "board :\n",
            " [[-1  0  1]\n",
            " [ 0  1  0]\n",
            " [ 1 -1 -1]]\n",
            "\n",
            "Game : 29\n",
            "-1 wins\n",
            "board :\n",
            " [[ 1  0  0]\n",
            " [ 1  0  1]\n",
            " [-1 -1 -1]]\n",
            "\n",
            "Game : 30\n",
            "1 wins\n",
            "board :\n",
            " [[-1  1  1]\n",
            " [ 0  1 -1]\n",
            " [ 1 -1 -1]]\n",
            "\n",
            "Game : 31\n",
            "1 wins\n",
            "board :\n",
            " [[-1  1 -1]\n",
            " [-1 -1  1]\n",
            " [ 1  1  1]]\n",
            "\n",
            "Game : 32\n",
            "-1 wins\n",
            "board :\n",
            " [[-1 -1 -1]\n",
            " [ 1  1 -1]\n",
            " [ 0  1  1]]\n",
            "\n",
            "Game : 33\n",
            "-1 wins\n",
            "board :\n",
            " [[-1  1 -1]\n",
            " [-1 -1  1]\n",
            " [-1  1  1]]\n",
            "\n",
            "Game : 34\n",
            "-1 wins\n",
            "board :\n",
            " [[ 1 -1  1]\n",
            " [ 1  0  0]\n",
            " [-1 -1 -1]]\n",
            "\n",
            "Game : 35\n",
            "-1 wins\n",
            "board :\n",
            " [[-1 -1  0]\n",
            " [ 1 -1  0]\n",
            " [ 1 -1  1]]\n",
            "\n",
            "Game : 36\n",
            "1 wins\n",
            "board :\n",
            " [[ 1  1  0]\n",
            " [-1  1 -1]\n",
            " [-1  1 -1]]\n",
            "\n",
            "Game : 37\n",
            "1 wins\n",
            "board :\n",
            " [[ 1 -1 -1]\n",
            " [-1  1  1]\n",
            " [ 0  0  1]]\n",
            "\n",
            "Game : 38\n",
            "1 wins\n",
            "board :\n",
            " [[ 1  0 -1]\n",
            " [ 1  1  1]\n",
            " [-1 -1  0]]\n",
            "\n",
            "Game : 39\n",
            "1 wins\n",
            "board :\n",
            " [[-1 -1  1]\n",
            " [ 1  1  1]\n",
            " [-1  1 -1]]\n",
            "\n",
            "Game : 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 61.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 wins\n",
            "board :\n",
            " [[-1 -1  1]\n",
            " [ 1 -1 -1]\n",
            " [ 1  1  1]]\n",
            "\n",
            "Game : 41\n",
            "1 wins\n",
            "board :\n",
            " [[-1 -1  1]\n",
            " [ 1 -1 -1]\n",
            " [ 1  1  1]]\n",
            "\n",
            "Game : 42\n",
            "\n",
            "Draw\n",
            "board :\n",
            " [[ 1 -1 -1]\n",
            " [-1  1  1]\n",
            " [ 1  1 -1]]\n",
            "\n",
            "Game : 43\n",
            "1 wins\n",
            "board :\n",
            " [[ 1 -1 -1]\n",
            " [-1  1  1]\n",
            " [ 0 -1  1]]\n",
            "\n",
            "Game : 44\n",
            "1 wins\n",
            "board :\n",
            " [[-1 -1  1]\n",
            " [-1  1  1]\n",
            " [ 1  1 -1]]\n",
            "\n",
            "Game : 45\n",
            "-1 wins\n",
            "board :\n",
            " [[-1  1 -1]\n",
            " [ 1  0 -1]\n",
            " [ 1  1 -1]]\n",
            "\n",
            "Game : 46\n",
            "1 wins\n",
            "board :\n",
            " [[ 1 -1  1]\n",
            " [-1  1 -1]\n",
            " [-1  0  1]]\n",
            "\n",
            "Game : 47\n",
            "\n",
            "Draw\n",
            "board :\n",
            " [[ 1 -1  1]\n",
            " [ 1 -1 -1]\n",
            " [-1  1  1]]\n",
            "\n",
            "Game : 48\n",
            "-1 wins\n",
            "board :\n",
            " [[ 0  0  0]\n",
            " [-1 -1 -1]\n",
            " [ 1  0  1]]\n",
            "\n",
            "Game : 49\n",
            "-1 wins\n",
            "board :\n",
            " [[-1 -1 -1]\n",
            " [ 1  0  1]\n",
            " [ 0  1 -1]]\n",
            "22 21 50\n",
            "Win percentage: Agent X 44.00%, Agent O 42.00%.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTUzmJs-09_u",
        "outputId": "99fb6082-9b45-4d0c-91d1-1b67d9aed249"
      },
      "source": [
        "board=Board()\n",
        "print(board.checkWinner())\n",
        "\n",
        "board.state = np.array(((-1,-1,-1), (0,0,0), (0,0,0)))\n",
        "print(board.checkWinner())\n",
        "print(board.getStateHash())\n",
        "pos = board.getAvailablePos()\n",
        "print(\"getAvailablePos\\n\", pos)\n",
        "print(\"checkGameEnded\\n\", board.checkGameEnded())\n",
        "\n",
        "\n",
        "list1 = board.getAvailablePos()\n",
        "print(\"shape\\n\",list1.shape[0])\n",
        "ch1= np.random.choice(list1.shape[0])\n",
        "print(\"random.choice\\n\",ch1)\n",
        "print(\"random action\\n\", list1[ch1])\n",
        "\n",
        "board.state = np.array(((-1,0,0), (0,1,0), (0,0,-1)))\n",
        "print(board.getStateHash())\n",
        "\n",
        "board.state = np.array(((-1,1,1), (1,1,1), (1,-1,-1)))\n",
        "print(board.checkGameEnded())\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "-1\n",
            "[[-1 -1 -1]\n",
            " [ 0  0  0]\n",
            " [ 0  0  0]]\n",
            "getAvailablePos\n",
            " [[1 0]\n",
            " [1 1]\n",
            " [1 2]\n",
            " [2 0]\n",
            " [2 1]\n",
            " [2 2]]\n",
            "checkGameEnded\n",
            " False\n",
            "shape\n",
            " 6\n",
            "random.choice\n",
            " 0\n",
            "random action\n",
            " [1 0]\n",
            "[[-1  0  0]\n",
            " [ 0  1  0]\n",
            " [ 0  0 -1]]\n",
            "True\n"
          ]
        }
      ]
    }
  ]
}