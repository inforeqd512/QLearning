{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_TicTacToe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxFlSGktl+Ou9OUs40qRcD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inforeqd512/QLearning/blob/main/RL_TicTacToe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09zQDJ8boz15"
      },
      "source": [
        "import numpy as np\n",
        "import pylab as plt"
      ],
      "execution_count": 790,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhMbEyXuXjS7"
      },
      "source": [
        "# Game\n",
        "\n",
        "Every simulation starts with a new Game _Board_. \n",
        "\n",
        "Each iteration of the simulation ends when the board is full or when any of the players wins.\n",
        "\n",
        "Every Player _Agent_ is setup with their _Policy Trainer_ and follows espilon greedy policy to learn best actions over several iterations of the simulation.\n",
        "\n",
        "**measure of success** : The Players become good at playing when the 'Draw' percentages are high.. ie. the first player to go learns to play in the middle or lears to play to Draw\n",
        "\n",
        "It takes approximately 5000 iterations to get to optimal q-values. At that time both players learn the game well enough to win approx 50% of the time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs_GTXqantDk"
      },
      "source": [
        "### Actors in the Game"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaQuOM1Mnu90"
      },
      "source": [
        "\n",
        "_Board_ is the environment. _State_ is the position of the board after a move from any player. Board also provides the possible set of actions. These are set of open positions on the board on which the player can put it's symbol. \n",
        "\n",
        "_Agent_ is the player playing the game. \n",
        "\n",
        "- Player playing $X$ will be denoted by symbol $1$\n",
        "- Player playing $O$ will be denoted by symbol $-1$\n",
        "\n",
        "Every _Agent_ has its own _Policy Trainer_ that learns the best action it should do in the _Board_ environment. \n",
        "\n",
        "The _Policy_Trainer_ follows is the epsilon greedy random policy to choose the next action. Based on this policy it learns the _Quality Action or Q-table_ for each player. Typically the Q-values of Player X are different to those of Player O. \n",
        "\n",
        "The _Policy_Trainer_ also needs to update the Q-values after performing the action and seeing the reward, so the _Agent_ delegates the choosing of the action and the performing of the action to it. \n",
        "\n",
        "\n",
        "_Action_ to be performed is chosen by the agent on the current state of the board. It is the next available position on the board that the agent can choose from. \n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKTADaJ5V2Wh"
      },
      "source": [
        "### Exploration and Exploitation\n",
        "\n",
        "Initially we explore more as we learn about the possible moves that can be made. Over multiple iterations we start exploiting what we've learnt so we decay the exploration factor epsilon. \n",
        "\n",
        "**Decaying epsilon-greedy**: Does the same as epsilon-greedy, however, the epsilon value starts out near 1, and decays over time according to γ to power of x where x represents the iteration the agent is in.  For γ 0.99 is used \n",
        "\n",
        "<sup>Source: [The Effect of the Exploration\n",
        "Strategy on an Agent’s Performance](https://theses.ubn.ru.nl/bitstream/handle/123456789/5216/Nieuwdorp,%20T._BSc_Thesis_2017.pdf?sequence=1) from Thijs Nieuwdorp</sup>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWBNNhH_Veiz"
      },
      "source": [
        "### Quality of Action \n",
        "\n",
        "**Q (dict)** - \n",
        "\n",
        "- key = state hash, \n",
        "- value = is an array representing 9 spaces in the tic tac toe grid,\n",
        "\n",
        "Starting from top row of grid, from left to right then next row from left to right etc, based on the grid position sent as input action, the array index is computed\n",
        "      \n",
        "eg \n",
        "- for 0,1 grid position = 0 * 3 (number of rows and cols in grid) + 1 == 1 so the index of 1.. ie at position 2 in the array\n",
        "- for 1,1 grid position = 1 * 3 + 1 == 4 so 0,1,2,3,4th position in the array \n",
        "\n",
        "beware!!! when using the default initialisation of parameter as {} then python allocates the same memory to both instances of PolicyTrainer. As a result both policies showed the same scores. Also showed win for 1 when should show only for -1.. The best way to find this happening is to initialise the game state to a board where only one player can win, but you'll see that both players get a non-zero Q-value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO0tWLLZnHm1"
      },
      "source": [
        "# Board"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtDnMMJ_nHP2"
      },
      "source": [
        "class Board:\n",
        "  \"\"\" Class that represents the game board of Tic Tac Toe \"\"\"\n",
        "\n",
        "  playerX = 1\n",
        "  playerO = -1\n",
        "\n",
        "  def __init__(self, rows=3, cols=3):\n",
        "    self.rows = rows\n",
        "    self.cols = cols \n",
        "    self.resetGame()\n",
        "\n",
        "  def resetGame(self):\n",
        "    \"\"\" resets the state of the board when needed eg, start of a simulation and after one of the players has won \"\"\"\n",
        "    self.state = np.zeros((self.rows, self.cols), dtype=np.int8)\n",
        "    # use below to test logic for one single win and change the learning rate to 1 to make equations simpler to test\n",
        "    # self.state = np.array(\n",
        "    #               [[ -1,  1,  0],\n",
        "    #               [-1,  1, -1],\n",
        "    #               [ 1, -1,  1]])\n",
        "    self.state = np.array(\n",
        "                  [[ 0,  0,  -1],\n",
        "                  [0,  1, -1],\n",
        "                  [ 0, 0,  0]])\n",
        "  def checkWinner(self):\n",
        "    \"\"\"  return winner symbol, if one exists. 0 if no winner\"\"\"\n",
        "\n",
        "    symbols = np.unique(self.state) #unique values , 0, 1, -1\n",
        "    symbols = symbols[np.nonzero(symbols)] #remove 0's\n",
        "    winning_symbol = 0 #no winner yet\n",
        "\n",
        "    for symbol in symbols:\n",
        "      #check rows\n",
        "      row = np.any(np.all(self.state == symbol, axis=1))\n",
        "\n",
        "      #check cols\n",
        "      col = np.any(np.all(self.state == symbol, axis=0))\n",
        "\n",
        "      #check diagonals\n",
        "      diag1 = np.array([self.state[0,0], self.state[1,1], self.state[2,2]])\n",
        "      diag1 = np.all(diag1 == symbol)\n",
        "\n",
        "      diag2 = np.array([self.state[2,0], self.state[1,1], self.state[0,2]])\n",
        "      diag2 = np.all(diag2 == symbol)\n",
        "\n",
        "      # Check if state has winner and return winner in that case\n",
        "      if row or col or diag1 or diag2:\n",
        "        winning_symbol = symbol\n",
        "        break\n",
        "  \n",
        "    return winning_symbol\n",
        "\n",
        "  def getAvailablePos(self):\n",
        "    \"\"\"  Get state positions that have no value ie zeros \"\"\"\n",
        "    return np.argwhere(self.state == 0)\n",
        "\n",
        "  def checkGameEnded(self):\n",
        "    \"\"\" Check if game has ended by observing if there any possible moves left\n",
        "    or there is a winner \"\"\"\n",
        "    ended = (len(self.getAvailablePos()) == 0) or (self.checkWinner() != 0)\n",
        "    return ended\n",
        "\n",
        "  def setPosition(self, x, y, symbol):\n",
        "    \"\"\"  Set state at position (x,y) with symbol \"\"\"\n",
        "    self.state[x,y] = symbol\n",
        "\n",
        "  def getStateHash(self):\n",
        "    \"\"\"  Get hash key of state \"\"\"\n",
        "    return np.array2string(self.state)\n",
        "  \n",
        "  def performAction(self, action_to_perform, symbol):\n",
        "    \"\"\"  Perform the action on current board \"\"\"\n",
        "    self.setPosition(action_to_perform[0], action_to_perform[1], symbol)\n",
        "    return self\n",
        "      "
      ],
      "execution_count": 791,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSWPabSDToIi"
      },
      "source": [
        "class Agent:\n",
        "  \"\"\" Class that represents the player \n",
        "      symbol is 1 for 'X' or -1 for 'O' \n",
        "      policy_trainer is initialised with epsilon greedy set to exploration_probability which is reduced overtime\"\"\"\n",
        "\n",
        "  def __init__(self, symbol, exploration_probability):\n",
        "    self.symbol = symbol\n",
        "    self.policy_trainer = PolicyTrainer(exploration_probability=exploration_probability, symbol=symbol)\n",
        "    return\n",
        "\n",
        "  def performActionPerPolicy(self, state_hash, possible_actions, current_state):\n",
        "    \"\"\" per the explore and exploitation with current epsilon \"\"\"\n",
        "    action = self.policy_trainer.chooseAction(state_hash, possible_actions)\n",
        "    self.policy_trainer.performAction(current_state, action)\n",
        "\n",
        "  def epsilonDecayPerIterations(self, num_iterations):\n",
        "    \"\"\" gradually reduces probabilities per iteration but not completely eliminate it\"\"\"\n",
        "    # Reduce probability to explore during training\n",
        "    # Do not remove completely \n",
        "    \n",
        "    # Decaying epsilon-greedy: Does the same as epsilon-greedy, however, the epsilon value starts out near 1,\n",
        "    # and decays over time according to γ to power of x where x represents the iteration the agent is in. \n",
        "    # For γ 0.99 is used per https://theses.ubn.ru.nl/bitstream/handle/123456789/5216/Nieuwdorp,%20T._BSc_Thesis_2017.pdf?sequence=1\n",
        "        \n",
        "    if self.policy_trainer.exploration_probability > 0.2:\n",
        "      decay = 0.99 ** num_iterations\n",
        "      self.policy_trainer.exploration_probability = decay\n",
        "\n",
        "  def updateScore(self):\n",
        "    \"\"\" measures how much is the q-values changing over iterations till finally they settle \"\"\"\n",
        "    self.policy_trainer.updateScore()\n"
      ],
      "execution_count": 792,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLEV9tTvYLQZ"
      },
      "source": [
        "class PolicyTrainer:\n",
        "  \"\"\"\n",
        "      exploration_probability (float) epsilon greedy value\n",
        "      learning_rate (float)\n",
        "      discount_factor (float)\n",
        "      Q (dict) - {state_hash : array of grid positions}\n",
        "  \"\"\"\n",
        "  def __init__(self, symbol, exploration_probability, learning_rate = 0.1, discount_factor = 0.9, grid_size = 3):\n",
        "    self.Q = {} #beware!!! when using the initiatlisation value a {} then python allocates the same one to both policies\n",
        "    self.learning_rate = 1.0 #learning_rate\n",
        "    self.discount_factor = discount_factor\n",
        "    self.exploration_probability = exploration_probability\n",
        "    self.grid_size = grid_size\n",
        "    self.scores = []\n",
        "    self.symbol = symbol\n",
        "    self.num_times_states_seen = {}\n",
        "    return\n",
        "\n",
        "  def getActionIndex(self, current_action):\n",
        "    \"\"\" Returns index in the action array position where the q value should be put \"\"\"\n",
        "    idx = self.grid_size * current_action[0] + current_action[1]\n",
        "    return idx\n",
        "\n",
        "  def getValueQ(self, current_state_hash, current_action):\n",
        "    \"\"\" Get the quality value of a given action in a given state,\n",
        "            returns 0 if the state-action pair has not been seen before.\n",
        "            Input is state hash key\n",
        "            and action as an array of position where the symbol should be put ie [0,1] for top row middle square\"\"\"\n",
        "    idx = self.getActionIndex(current_action)\n",
        "    q_value = 0 \n",
        "    if current_state_hash in self.Q:\n",
        "      q_value = self.Q[current_state_hash][idx]\n",
        "\n",
        "    return q_value\n",
        "\n",
        "  def setValueQ(self, current_state_hash, current_action, value):\n",
        "    \"\"\" Set value in Q \n",
        "    if this is the first time the state is seen, then the value is the array representing the grid\"\"\"\n",
        "    idx = self.getActionIndex(current_action)\n",
        "    if current_state_hash in self.Q:\n",
        "      self.Q[current_state_hash][idx] = value\n",
        "    else:\n",
        "      self.Q[current_state_hash] = np.zeros(self.grid_size * self.grid_size)\n",
        "      self.Q[current_state_hash][idx] = value\n",
        "    return\n",
        "\n",
        "  def rewardFunction(self, current_state_hash, next_state):\n",
        "    \"\"\" when the chosen action is performed on a state, then we get a new state\n",
        "    and associated reward from this transition is computed here\n",
        "    if next state is winner then reward is 1 else 0 \"\"\"\n",
        "    winner = next_state.checkWinner()\n",
        "    reward = 0 \n",
        "    if winner == self.symbol:\n",
        "      reward = 1\n",
        "    \n",
        "    # if reward == 1:\n",
        "    # if self.symbol == 1:\n",
        "    #   print(\"***rewardFunction\")\n",
        "    #   print(\"current_state_hash :  \\n\", current_state_hash)\n",
        "    #   print(\"next state hash :  \\n\", next_state.getStateHash())\n",
        "    #   print(\"symbol :  \", self.symbol)\n",
        "    #   print(\"reward :  \", reward)\n",
        "\n",
        "    return reward\n",
        "\n",
        "  def chooseAction(self, state_hash, possible_actions):\n",
        "    \"\"\" choose action per epsilon greedy explore/exploit policy \"\"\"\n",
        "    #Explore\n",
        "    if random.random() < 0.2: #self.exploration_probability:\n",
        "      action = self.chooseRandomAction(possible_actions)\n",
        "    else:\n",
        "      #Exploit\n",
        "      action = self.chooseBestAction(state_hash, possible_actions)\n",
        "    return action\n",
        "\n",
        "  def chooseRandomAction(self, possible_actions):\n",
        "    \"\"\" choose random action from list of possible actions in a state \"\"\"\n",
        "    random_idx = np.random.choice(possible_actions.shape[0])\n",
        "    action_pos = possible_actions[random_idx]\n",
        "    return action_pos\n",
        "\n",
        "  def chooseBestAction(self, current_state_hash, possible_actions):\n",
        "    \"\"\" Get best action given a set of possible actions in a given state \"\"\"\n",
        "    # Pick a random action at first\n",
        "    random_idx = np.random.choice(possible_actions.shape[0])\n",
        "    best_action = possible_actions[random_idx]\n",
        "\n",
        "    # Find action that given largest Q in given state\n",
        "    maxQ = 0 \n",
        "    for action in possible_actions:\n",
        "      tmpQ = self.getValueQ(current_state_hash, action)\n",
        "      if maxQ < tmpQ:\n",
        "        maxQ = tmpQ\n",
        "        best_action = action\n",
        "\n",
        "    return best_action\n",
        "\n",
        "  def getMaxQ(self, next_state):\n",
        "    \"\"\" go through all possible actions in a state \n",
        "    and pick the one with the highest quality value \"\"\"\n",
        "    actions = next_state.getAvailablePos()\n",
        "    state_hash = next_state.getStateHash()\n",
        "\n",
        "    # Find action that given largest Q in given state\n",
        "    maxQ = 0 \n",
        "    for action in actions:\n",
        "      tmpQ = self.getValueQ(state_hash, action)\n",
        "      if maxQ < tmpQ:\n",
        "        maxQ = tmpQ\n",
        "\n",
        "    return maxQ\n",
        "\n",
        "  def performAction(self, current_state, current_action):\n",
        "    \"\"\" Implements Q-learning iterative algorithm \n",
        "    we use a high learning_rate to give more value to the immediate reward so that we can see the values settle within 5000 iterations\n",
        "    \"\"\"\n",
        "    current_state_hash = current_state.getStateHash()\n",
        "\n",
        "    # Get current Q Value\n",
        "    currentQ = self.getValueQ(current_state_hash, current_action)\n",
        "\n",
        "    next_state = current_state.performAction(current_action, self.symbol)\n",
        "\n",
        "    newQ = (1 - self.learning_rate) * currentQ\n",
        "    newQ += self.learning_rate * (self.rewardFunction(current_state_hash, next_state) + self.discount_factor * self.getMaxQ(next_state))\n",
        "    self.setValueQ(current_state_hash, current_action, newQ)\n",
        "\n",
        "    if current_state_hash in self.num_times_states_seen:\n",
        "      arr = self.num_times_states_seen[current_state_hash]\n",
        "      if np.any(arr == current_action):\n",
        "        #do nothing\n",
        "        1\n",
        "      else:\n",
        "        arr.append(current_action)\n",
        "        self.num_times_states_seen[current_state_hash] = list1\n",
        "    else:\n",
        "      self.num_times_states_seen[current_state_hash] = [current_action]\n",
        "\n",
        "    # if self.symbol == 1:\n",
        "    #   print(\"current_action :\", current_action, \"newQ : \", newQ)\n",
        "      # print(\"\\nperformAction\\n\", \"\\nsymbol :\\n\", self.symbol, \"\\ncurrentQ :\\n\", currentQ, \"\\ncurrent_state_hash :\\n\", current_state_hash, \"\\ncurrent_action :\\n\", current_action, \"\\nnewQ :\\n\", newQ)\n",
        "\n",
        "  def updateScore(self):\n",
        "    #compute score\n",
        "    score = 0\n",
        "    list1 = list(self.Q.values())\n",
        "    score = np.sum(list1)\n",
        "    self.scores.append(score)\n"
      ],
      "execution_count": 793,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHcSV1SNQcsM"
      },
      "source": [
        "# Q-Learning : Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZIXiKY_RtYf"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import random"
      ],
      "execution_count": 794,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW2Kf4WpnDLu"
      },
      "source": [
        "def simulate(iterations):\n",
        "  \"\"\" iterations (int) \"\"\"\n",
        "\n",
        "  # Construct game board\n",
        "  game = Board()\n",
        "\n",
        " # Epsilon-greedy \n",
        "  exploration_probability = 1.0\n",
        "\n",
        "  # Initiatlise players\n",
        "  playerX = Agent(Board.playerX, exploration_probability)\n",
        "  playerO = Agent(Board.playerO, exploration_probability)\n",
        "\n",
        "  # Counters for wins of each agent and total number of games\n",
        "  nbr_wins_playerX = 0\n",
        "  nbr_wins_playerO = 0\n",
        "  nbr_games = 0\n",
        "\n",
        "  # Pick current player\n",
        "  current_player = playerX\n",
        "\n",
        "  for i in tqdm(range(iterations)):\n",
        "\n",
        "    # play full games in each iteration\n",
        "    while not game.checkGameEnded():\n",
        "      possible_actions = game.getAvailablePos()\n",
        "      state_hash = game.getStateHash()\n",
        "         \n",
        "      current_player.performActionPerPolicy(state_hash, possible_actions, game)\n",
        "\n",
        "      # Check if there is a winner\n",
        "      winner = game.checkWinner() # Returns 0 if there is no winner\n",
        "      if winner != 0: #winner is when it's 1 or -1\n",
        "        if winner == playerX.symbol:\n",
        "            nbr_wins_playerX += 1\n",
        "        else:\n",
        "            nbr_wins_playerO += 1\n",
        "\n",
        "      # Swap player\n",
        "      if current_player == playerX:\n",
        "          current_player = playerO\n",
        "      else:\n",
        "          current_player = playerX\n",
        "\n",
        "    #when the full game is finished or there is a winner, then reset game and increment and print metrics\n",
        "    nbr_games += 1\n",
        "    playerX.updateScore()\n",
        "    playerO.updateScore()\n",
        "    playerX.epsilonDecayPerIterations(nbr_games)\n",
        "    playerO.epsilonDecayPerIterations(nbr_games)\n",
        "    \n",
        "    game.resetGame()\n",
        "\n",
        "  # Print outcome\n",
        "  print(\"\\n\\nWins: Agent X : \", nbr_wins_playerX, \"Wins: Agent O : \", nbr_wins_playerO, \"Total Games : \", nbr_games)    \n",
        "  print(\"\\nWin percentage: Agent X {:.2%}, Agent O {:.2%}.\".format(nbr_wins_playerX/nbr_games, nbr_wins_playerO/nbr_games))\n",
        "  print(\"\\nDraw percentage: {:.2%} \".format((nbr_games - nbr_wins_playerX - nbr_wins_playerO)/nbr_games))\n",
        "\n",
        "  return (playerX, playerO)\n",
        "\n"
      ],
      "execution_count": 795,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKnSaWTQaJcy"
      },
      "source": [
        "# Q-Learning : Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5ynzHkISBxX",
        "outputId": "701c539e-135a-4d56-b4e0-319b5db0614e"
      },
      "source": [
        "(playerX, playerO) = simulate(5000)"
      ],
      "execution_count": 796,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:09<00:00, 505.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Wins: Agent X :  663 Wins: Agent O :  4252 Total Games :  5000\n",
            "\n",
            "Win percentage: Agent X 13.26%, Agent O 85.04%.\n",
            "\n",
            "Draw percentage: 1.70% \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnLeAGYcaT13"
      },
      "source": [
        "### How the episodes settle the Q-values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "m6Uks9AbHKG2",
        "outputId": "2021c0bb-a5b7-49dd-9eb0-15b0461e5921"
      },
      "source": [
        "plt.plot(playerX.policy_trainer.scores)\n",
        "plt.show()"
      ],
      "execution_count": 797,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eApg9sDu1si-",
        "outputId": "7b9e8682-6950-462d-b77a-df1e4166a8bb"
      },
      "source": [
        "plt.plot(playerO.policy_trainer.scores)\n",
        "plt.show()"
      ],
      "execution_count": 798,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAb3ElEQVR4nO3de3xV5Z3v8c8vCblwTUhCiAQIl4BQRcB4xQuKWmu1ao9aW2fKWM/h9Wqt1nZOrc68Tjszp522To/Wjo4tXmZ0xlZt6xTUIiLiBasgd7nKPYQEEi4JkAtJdp7zx15gEoKB7Mvae+3v+/XKK3s9a2Xv35Nsvz48e631mHMOEREJljS/CxARkehTuIuIBJDCXUQkgBTuIiIBpHAXEQmgDL8LACgoKHClpaV+lyEiklSWL1++zzlX2N2+hAj30tJSli1b5ncZIiJJxcx2nmyfpmVERAJI4S4iEkAKdxGRAFK4i4gEkMJdRCSAegx3M3vGzGrMbG2HtsFmtsDMNnvf87x2M7NfmdkWM1tjZlNjWbyIiHTvVEbu/wFc26XtAWChc64MWOhtA3wBKPO+ZgFPRKdMERE5HT2e5+6ce9fMSrs03whM9x4/C7wN/MBrf86F7yP8oZnlmlmxc646WgWLiETbkaNtPPuXHRxtDcX9tWdMKOKc4blRf97eXsRU1CGw9wBF3uNhwK4Ox1V6bSeEu5nNIjy6Z8SIEb0sQ0Qkcu9+Usu/zN8EgFl8X3vIwOyECvfjnHPOzE57xQ/n3GxgNkB5eblWDBGRqPrd0gqq65pO6dj11YcA+PDBGQwdlB3LsuKmt+G+99h0i5kVAzVe+25geIfjSrw2EZG42X/kKA++/DFw6iPxkrwcBvfLjGFV8dXbcJ8LzAR+5n2f06H922b2AnABUK/5dhGJpSff3cb7W/d1amtqCc+dP3r7ZG6cPMyPsnzXY7ib2e8If3haYGaVwI8Ih/pLZnYXsBO4zTv8z8B1wBagEbgzBjWLiBz3zPvbOdrWzvC8nE7t548azNQReT5V5b9TOVvmqyfZNaObYx1wd6RFiUjwPPbWZj7acTDqz1t7+Chfv6iUH94wMerPncwS4pa/IhJ8Ty3eTkZaGsO6jLAjdXbJIK48c0hUnzMIFO4i8pkeWfAJK3fVRfw89U2tfGv6GL7/+TOjUJX0ROEuIp/pyfe20T8rgzNyIxtxTx2Rx/TxGmHHi8JdRPjhnLW8srqq232NLSG+efkY7plRFueqJBIKd5EUd6ChhXc+qSW3byaXlhWcsD89zbhpSmqeTpjMFO4iKaxifyPTf7GIdgczLxrJP954lt8lSZQo3EVSTO3hozR7N8hauesg7Q7uvmIMf3PxKJ8rk2hSuIukkDWVdXzpsfdPaL9x8jAKB2T5UJHEisJdJEUs33mQeR+H7wZy/7XjGTIgfIOsgdkZlA3p72dpEgMKd5EU0NQS4rbffECo3dEn3bjj/JEM6tvH77IkhhTuIgG2peYIG/ccor6plVC747tXjeOr5w9XsKcAhbtIgN39/Ao27T18fHvKiFyGDAzG/crlsyncRQJi057DvLyyEjosfVNxoJEbzjmDe68cS1ZGOiPy+/pXoMSVwl0kIP79/e288NEusvt8uu59mhmXjyukrGiAj5WJHxTuIkmmuTXEr9/ZSmNL58Wcl+44QNmQ/iz43uU+VSaJROEukmSW7TjIL9/cTGZGGuld1pDTbQLkGIW7SIJbuGEvS7YfOL69fV8DAK/ecwnjNN0iJ6FwF0lwP523ke37GshM/3QuvSQvJ+Jb8EqwKdxFYuip97bx5oa9ET1Hxf5GvnLecP755rOjVJWkAoW7SAz9dmkFh5paGV3Y+8v7p4zI5QtnDY1iVZIKFO4ivRRqd9zwr4vZub/hpMc0tIS444IR/ESjbokzhbtIL22rPcL66kNcPCaficUDuz3GDG4tHx7nykQU7iK9cqChhasfeReAr10wgusnneFzRSKdpfV8iIh0FGp3rNh5EICbJp/B1ROLfK5I5EQauYucpocXbOLxRVsB+NoFI8nKSPe5IpETaeQuchraQu28t3kfBf2zeHpmOeUj8/wuSaRbCneR0/DTeRtZU1nP8ME5zJhQRFqa9fxDIj7QtIxIDzbuOcT6qkMAfLQjfBuAh2+b7GdJIj1SuIv04Nu/XcmWmiPHt6+eWMSogn4+ViTSM4W7SAcrKg4yd1VVp7bKg43cPGUY911VBsDQQVrJSBKfwl2kgyff3cb8dXvon/XpfxrZfdKZPr6QkfkarUvyiCjczey7wP8kvLDXx8CdQDHwApAPLAf+2jnXEmGdIjFRXd/E0+9tp609vDbd6l11TB2Rxx++ebHPlYlEptfhbmbDgHuBic65JjN7CbgduA54xDn3gpn9GrgLeCIq1YpE2Wtrqnlq8XYGZmdg3sIXF47O97kqkchFOi2TAeSYWSvQF6gGrgS+5u1/FvgHFO6SoOat3QPA6h9dczzcRYKg1+e5O+d2A78AKgiHej3haZg651ybd1gl0O26X2Y2y8yWmdmy2tra3pYhEpHdB5sAFOwSOJFMy+QBNwKjgDrg98C1p/rzzrnZwGyA8vJy19s6RLp67oMdbO1w6uJnOdDYwt9cXBrTekT8EMm0zFXAdudcLYCZvQxMA3LNLMMbvZcAuyMvU+TUtLS188M568juk0Z2n57v+dI/K4PyUt1CQIInknCvAC40s75AEzADWAYsAm4hfMbMTGBOpEWKdLWi4iB/WnniuKGlrR2AB78wgZkakUsK63W4O+eWmNkfgBVAG7CS8DTLa8ALZvZjr+3paBQq0tHTi7cz7+NqBuX0OWHfkAFZnF0yyIeqRBJHRGfLOOd+BPyoS/M24PxInlcEoLk1xLeeX8GBhhMvk9hae4Rzhufy39+a5kNlIolPV6hKwtqxv4G3NtYwsXggBQOyOu2bMiKP6ycV+1SZSOJTuEvCajgaAuD+a8czffwQn6sRSS66n7skrMaW8OUS/bI0BhE5XQp3SVjLvXVKc07hlEYR6UzhLgnrcHN45D52SH+fKxFJPvr3rsTdQ69v5Il3tvZ4nHMwLDfnlC5GEpHOFO4Sd2sq6ykemM0t55b0eOyUEbp6VKQ3FO4SFxX7G7nuV+9x5Gh4quWK8YV875rxPlclElwKd4mLrbVHOHK0jdvPG86QgdlcNUGnNorEksJdIvbQ6xv5t7d7nkMHmHXZaEYX6gNSkVhTuMspqT18lOr6pm73/WXrfobl5vQ4h57fP5NRBVqHVCQeFO5ySm7+t/epPNh9uANc+7mhfPfqcXGsSEQ+i8JdehRqd1QebOL6ScXcPKXbhbWYVJIb56pE5LMo3KVHKyvCV4qOLxrAjAlFPlcjIqdCV6hKjx5e8AmAbt4lkkQU7vKZGlva+MvW/QCMLOjrczUicqo0LSMntWhjDT9/fSMAP/3y2QzMPnHVIxFJTBq5y0kt2lTDttoGvnh2MZeWFfhdjoicBo3chUcWfMIb6/ee0F5V10TRoCwev2OqD1WJSCQU7sIrq6toag1x1rDOi0qX5OVoxC6SpBTuKez/vbGJBev3UnGgkVvLh/PTL5/td0kiEiUK9xQ2d3UVrW3tzJgw5KQXJ4lIclK4p4g99c188/nlNLWEjrdVHmzi9vOG85ObNWIXCRqFe4pYXVnHyoo6Lh6Tz4Ds8J+9NL+fRuwiAaVwTxE/nxc+X/2hWyZRkqeLkUSCTuEeUK2hdtqd+3S7vZ2B2RkMy83xsSoRiReFewC9v2UfX39mKaF216l95kUjMTOfqhKReFK4JynnHOuqDtHcGjph38INNYTaHffOKCMrI3wRshncMOmMeJcpIj5RuCepdzfvY+YzS0+6P6dPOt+ZUUZ6mkbqIqlI4Z5ENu89TF1TKwBLtoXv1PjLr0wmv3/mCccWD8pRsIukMIV7kqiqa+LqR97t1NYn3bh6YhH9svRnFJHOIkoFM8sFngLOAhzwDWAT8CJQCuwAbnPOHYyoSuFAQwsA911VRvnIwQAMGZilYBeRbkV6y99Hgdedc2cC5wAbgAeAhc65MmChty0ROtrWDsDk4blcUlbAJWUFjCsa4HNVIpKoeh3uZjYIuAx4GsA51+KcqwNuBJ71DnsWuCnSIgXe3BC+JW92n3SfKxGRZBDJyH0UUAv8u5mtNLOnzKwfUOScq/aO2QN0u6Kymc0ys2Vmtqy2tjaCMlLDE29vBeCMQboISUR6FsmEbQYwFbjHObfEzB6lyxSMc86Zmevuh51zs4HZAOXl5d0ek6raQu38+LUN1DWG59mP/XLuvXIsI/J16wAR6Vkk4V4JVDrnlnjbfyAc7nvNrNg5V21mxUBNpEWmmi21R/iPv+ygcEAWfTPD0zBjCvtx6bhCnysTkWTR63B3zu0xs11mNt45twmYAaz3vmYCP/O+z4lKpSlk7e5DQPgc9mljtRKSiJy+SM+juwd43swygW3AnYTn8V8ys7uAncBtEb5Gyqn3LlQaVdDP50pEJFlFFO7OuVVAeTe7ZkTyvKlu1a46AIoGZvtciYgkK10Bk0CcczS0hDh21wDdPkBEekvhnkD+z5y1/NeHFQBMLB7oczUikswU7glkXdUhSvP7cscFIzm3NM/vckQkiSncE0RdYwsrK+q4Ynwh/+uy0X6XIyJJLtJ7y0iU7NzfCEB56WCfKxGRIFC4J4DDza3c+Pj7AJw7UtMxIhI5hXsC2HckfJuBC0cPVriLSFQo3BNAw9E2AO6cNoo+6fqTiEjk9IGqT9rbHYe9UK89chSAfpn6c4hIdChNfHLfi6uYu7qqU9uAbP05RCQ6lCY+ONTcyqY9hzlz6ABuLR8OhIP97GGDfK5MRIJC4R5nz32wgx/OWQfAreeWcNclo/wtSEQCSeEeZ6t31ZPTJ53vf348V0/sdpEqEZGIKdzjaEXFQf64opLiQdl8QyN2EYkhnXcXRzv3NwDwt9eM97kSEQk6hXscNbaEALi0TKsriUhsKdzjqK4xvMLSsXVRRURiReEeJ845/mX+JgD66mIlEYkxpUwcfFxZT3V9EwCXjSvUCksiEnMK9xirOdzMDY8tPr59y7klPlYjIqlC4R5jqyrCi13/4NozubSsQMvniUhcKNxj7B/mhq9GvXD0YM7S7QVEJE4U7jFwbMk8gMNH27hg1GAmD8/1uSoRSSUK9xj4yWsb+P3yyuPbl4wtwEwfoopI/CjcY+BAQwujC/rx8Fcmk2YwQfPsIhJnCvcYaGwJMbhfpqZiRMQ3uogpBhpbQ+ToKlQR8ZHCPQaaWtp0iwER8ZXCPYrWVdUz67ll7DrQpPVQRcRXCvcomr9uL2+s38uYIf24csIQv8sRkRSm4WUULd2+n36Z6bx6z6V+lyIiKS7ikbuZpZvZSjN71dseZWZLzGyLmb1oZpmRl5k8Qs75XYKISFSmZb4DbOiw/XPgEefcWOAgcFcUXiPhvbammg+3HeCSsVqIQ0T8F1G4m1kJ8EXgKW/bgCuBP3iHPAvcFMlrJIsXPqoA4Nqzin2uREQk8pH7L4H7gXZvOx+oc861eduVwLDuftDMZpnZMjNbVltbG2EZ/lu9q47LxhXqlr4ikhB6He5mdj1Q45xb3pufd87Nds6VO+fKCwsLe1tGwuiTnkZLW8jvMkREgMjOlpkGfMnMrgOygYHAo0CumWV4o/cSYHfkZSaOP63czZ9WndiluqZWzinR7QZEJDH0euTunHvQOVfinCsFbgfecs7dASwCbvEOmwnMibjKBPLbpRUs23GQgw0tnb4mlQzi8vHJ/y8QEQmGWJzn/gPgBTP7MbASeDoGrxEXv1tawetr93Rq21B1iAtH5/PUzHKfqhIR6VlUwt059zbwtvd4G3B+NJ7Xb//5wU521zVRWtDveNvoIf354qShPlYlItIzXaF6Eu9trmV99SG+PGUYD39lst/liIicFt1b5iTmrwtPx/zVRSN9rkRE5PQp3Lvx6poq/uvDCvplpjN1RJ7f5YiInDaFexfOOZZuPwDAk/rQVESSlMK9iz+u2M1zH+wkKyONi8foPjEikpwU7h20htpZu7segP+86wKfqxER6T2dLdPB915azSurq8jvl8n5owb7XY6ISK9p5O453NzKlpojnDl0gC5QEpGkp3AHQu2Oyx5axIbqQ0woHsgUnSEjIklO4Q4caW7jYGMrN08Zxv3Xjve7HBGRiCncgcq6RgCmjsyjeFCOz9WIiEQu5cO95nAzW2qOAFCSp2AXkWBI6bNl1u6u5/p/XXx8+wyN2kUkIFI63DftOQzA9z8/nvFFAxhX1N/nikREoiOlw313XRMAt5aXMGRAts/ViIhET0rPuS/esg+Awv5ZPlciIhJdKRvuew81H79BmJn5XI2ISHSlZLg3t4Z4bU01AA/9j0k+VyMiEn0pGe6vrK7in15dD8CE4oE+VyMiEn0p+YHqWxtrAFj0v6czqsP6qCIiQZFyI/e2UDvz1oaX0CvN7+tzNSIisZFy4b626hAA371qnD5IFZHASrlwX7hhLwDTxub7XImISOykXLg3tYTom5lOeakW4xCR4Eq5cG9oCdE3MyU/RxaRFJJS4f7Guj18tOMA/bLS/S5FRCSmUircfzZvIzv2NXCepmREJOBSYn4i1O547K0tVNc3c9t5w/nnm8/2uyQRkZhKiZH7pj2HeeTNT0gzOFfro4pICgj8yH13XRNPvLMVgCdnlnPxmAKfKxIRib3Aj9znrqrildVVDMvNYWyhFuMQkdTQ63A3s+FmtsjM1pvZOjP7jtc+2MwWmNlm77uv8yBzVu0mzWDxD65gyEAtyCEiqSGSkXsb8LfOuYnAhcDdZjYReABY6JwrAxZ6277ZWnuENDPdakBEUkqvw905V+2cW+E9PgxsAIYBNwLPeoc9C9wUaZG9cbQtxC/mb6I15Lh3RpkfJYiI+CYqc+5mVgpMAZYARc65am/XHqDoJD8zy8yWmdmy2traaJTRyZrKeh5btIVBOX04Z3hu1J9fRCSRRRzuZtYf+CNwn3PuUMd9zjkHuO5+zjk32zlX7pwrLywsjLSMTj7acYDfvLMNgOe+cT6Xj4vu84uIJLqIwt3M+hAO9uedcy97zXvNrNjbXwzURFbi6Zv97jbe2riX0vy+lOZrMQ4RST2RnC1jwNPABufcwx12zQVmeo9nAnN6X17vNLa0MXVEHm9//woG9e0T75cXEfFdJBcxTQP+GvjYzFZ5bX8H/Ax4yczuAnYCt0VW4ulrOBpiYI5CXURSV6/D3Tm3GDjZ+YUzevu8kXp68XZ27m/gglFajENEUlegrlBtbg3xf19dz9G2ds4bpTs/ikjqCtS9Zd5YH15C7++um8BfXTjS52pERPwTqJH7yysqATh72CCfKxER8Vdgwv2TvYd5e1MtF4/J10VLIpLyAhPuL360C4BpY3VLXxGRwIT7K6urGDowm7uvGOt3KSIivgtEuDvnqDl8lDTd+FFEBAhIuG/f1wDA1y8u9bcQEZEEEYhwf3/LPgBGF+g+MiIiEJBw37jnMACX6e6PIiJAQML9mOw+6X6XICKSEAIR7i1t7RQP0vqoIiLHBCLcj7a1k5URiK6IiERF0ifirgONzF1dRVaGpmRERI5J+nBfU1kP6MpUEZGOkj7cG1vaALhzWqm/hYiIJJAAhHsIgL6ZmpYRETkm6cN9/ro9APTNDNSt6UVEIpL04d7W7gDI0chdROS4pA/35tYQ08frylQRkY6SPtybWkKabxcR6SLpw72xJaTbDoiIdJH04d7cGiJH4S4i0klSh3t7u2N/Q4vCXUSki6QO9wbvAqZjZ8yIiEhYUod7U2v4AqYxQ/r7XImISGJJ6nBvbmkH0LSMiEgXSR3ux0buCncRkc6CEe6ZSd0NEZGoS+pUbPbCXee5i4h0ltTh3qRwFxHpVkzC3cyuNbNNZrbFzB6IxWsAzPu4GoBsrcIkItJJ1O+Ta2bpwOPA1UAl8JGZzXXOrY/2a101oYj0tDTG6lRIEZFOYnET9POBLc65bQBm9gJwIxD1cL/mc0O55nNDo/20IiJJLxbTMsOAXR22K722TsxslpktM7NltbW1MShDRCR1+faBqnNutnOu3DlXXlio+7GLiERTLMJ9NzC8w3aJ1yYiInESi3D/CCgzs1FmlgncDsyNweuIiMhJRP0DVedcm5l9G5gPpAPPOOfWRft1RETk5GJxtgzOuT8Df47Fc4uISM+S+gpVERHpnsJdRCSAzDn/VzEys1pgZy9/vADYF8VykoH6nBrU59QQSZ9HOue6PZc8IcI9Ema2zDlX7ncd8aQ+pwb1OTXEqs+alhERCSCFu4hIAAUh3Gf7XYAP1OfUoD6nhpj0Oenn3EVE5ERBGLmLiEgXCncRkQBK6nCP13J+8WBmz5hZjZmt7dA22MwWmNlm73ue125m9iuv32vMbGqHn5npHb/ZzGb60ZdTYWbDzWyRma03s3Vm9h2vPch9zjazpWa22uvzP3rto8xside3F70b7mFmWd72Fm9/aYfnetBr32Rmn/enR6fOzNLNbKWZveptB7rPZrbDzD42s1Vmtsxri+972zmXlF+Eb0q2FRgNZAKrgYl+1xVBfy4DpgJrO7Q9BDzgPX4A+Ln3+DpgHmDAhcASr30wsM37nuc9zvO7byfpbzEw1Xs8APgEmBjwPhvQ33vcB1ji9eUl4Hav/dfAN73H3wJ+7T2+HXjRezzRe79nAaO8/w7S/e5fD33/HvBb4FVvO9B9BnYABV3a4vre9v2XEMEv7yJgfoftB4EH/a4rwj6Vdgn3TUCx97gY2OQ9/g3w1a7HAV8FftOhvdNxifwFzCG87m5K9BnoC6wALiB8dWKG1378fU34zqoXeY8zvOOs63u943GJ+EV4TYeFwJXAq14fgt7n7sI9ru/tZJ6WOaXl/JJckXOu2nu8ByjyHp+s70n5O/H+6T2F8Eg20H32pidWATXAAsIj0DrnXJt3SMf6j/fN218P5JNkfQZ+CdwPtHvb+QS/zw54w8yWm9ksry2u7+2Y3PJXos8558wscOetmll/4I/Afc65Q2Z2fF8Q++ycCwGTzSwX+G/gTJ9Liikzux6occ4tN7PpftcTR5c453ab2RBggZlt7LgzHu/tZB65p8JyfnvNrBjA+17jtZ+s70n1OzGzPoSD/Xnn3Mtec6D7fIxzrg5YRHhKItfMjg20OtZ/vG/e/kHAfpKrz9OAL5nZDuAFwlMzjxLsPuOc2+19ryH8P/HzifN7O5nDPRWW85sLHPuEfCbheelj7V/3PmW/EKj3/rk3H7jGzPK8T+Kv8doSjoWH6E8DG5xzD3fYFeQ+F3ojdswsh/BnDBsIh/wt3mFd+3zsd3EL8JYLT77OBW73ziwZBZQBS+PTi9PjnHvQOVfinCsl/N/oW865Owhwn82sn5kNOPaY8HtyLfF+b/v9wUOEH1pcR/gsi63A3/tdT4R9+R1QDbQSnlu7i/Bc40JgM/AmMNg71oDHvX5/DJR3eJ5vAFu8rzv97tdn9PcSwvOSa4BV3td1Ae/zJGCl1+e1wA+99tGEg2oL8Hsgy2vP9ra3ePtHd3iuv/d+F5uAL/jdt1Ps/3Q+PVsmsH32+rba+1p3LJvi/d7W7QdERAIomadlRETkJBTuIiIBpHAXEQkghbuISAAp3EVEAkjhLiISQAp3EZEA+v/ZEBc0Jn02lAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajuzJOhcanUw"
      },
      "source": [
        "### Trained Q-values for Player X\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u07cS-U5muRf"
      },
      "source": [
        "float_formatter = \"{:.2f}\".format\n",
        "np.set_printoptions(formatter={'float_kind':float_formatter})"
      ],
      "execution_count": 799,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0j5qISZP1Nd",
        "outputId": "e5a31531-26bb-440d-a282-c8760fe4782f"
      },
      "source": [
        "playerX.policy_trainer.num_times_states_seen"
      ],
      "execution_count": 800,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 0 -1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 0 -1  1]]': [array([0, 0])], '[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [ 0  1 -1]\\n [-1  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [ 0  1 -1]\\n [-1  0  1]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [ 0  1 -1]\\n [-1  1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [ 0  1 -1]\\n [-1 -1  1]]': [array([0, 0])], '[[ 0  0 -1]\\n [ 1  1 -1]\\n [ 0 -1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [ 1  1 -1]\\n [-1  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [ 1  1 -1]\\n [-1 -1  1]]': [array([0, 1])], '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 0  0  1]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 0  1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 0 -1  1]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 1 -1  0]]': [array([0, 0])], '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 1 -1  1]]': [array([0, 1])], '[[ 0  0 -1]\\n [-1  1 -1]\\n [-1  1  1]]': [array([0, 1])], '[[ 0  1 -1]\\n [ 0  1 -1]\\n [ 0 -1  0]]': [array([2, 0])], '[[ 0  1 -1]\\n [ 0  1 -1]\\n [-1  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  1 -1]\\n [ 0  1 -1]\\n [-1 -1  1]]': [array([0, 0])], '[[ 0  1 -1]\\n [ 1  1 -1]\\n [-1 -1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 0 -1  0]]': [array([2, 0])], '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 0 -1  1]]': [array([0, 0])], '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 1 -1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0  1 -1]\\n [-1  1 -1]\\n [-1  0  1]]': [array([0, 0])], '[[ 0  1 -1]\\n [-1  1 -1]\\n [-1 -1  1]]': [array([0, 0])], '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 0  0  1]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 0  1  0]]': [array([2, 0])], '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 0 -1  1]]': [array([1, 0])], '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 1  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': [array([0, 0])], '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 1 -1  1]]': [array([1, 0])], '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [-1  0  1]]': [array([1, 0])], '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [-1  1  1]]': [array([0, 0])], '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [ 0  0  0]]': [array([2, 0])], '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [ 0 -1  1]]': [array([0, 0])], '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [ 1 -1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [-1  0  1]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0 -1 -1]\\n [-1  1 -1]\\n [ 0  1  1]]': [array([2, 0])], '[[ 0 -1 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': [array([2, 1])], '[[ 0 -1 -1]\\n [-1  1 -1]\\n [ 1  0  1]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0 -1 -1]\\n [-1  1 -1]\\n [ 1  1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 0 -1 -1]\\n [-1  1 -1]\\n [ 1 -1  1]]': [array([0, 0])], '[[ 1  0 -1]\\n [ 0  1 -1]\\n [ 0 -1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 1  0 -1]\\n [ 0  1 -1]\\n [-1  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 1  0 -1]\\n [ 0  1 -1]\\n [-1 -1  0]]': [array([1, 0])], '[[ 1  0 -1]\\n [ 1  1 -1]\\n [-1 -1  0]]': [array([2, 2])], '[[ 1  0 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 1  0 -1]\\n [-1  1 -1]\\n [ 0 -1  0]]': [array([2, 2])], '[[ 1  0 -1]\\n [-1  1 -1]\\n [ 1 -1  0]]': [array([2, 2])], '[[ 1  1 -1]\\n [ 0  1 -1]\\n [-1 -1  0]]': [array([1, 0])], '[[ 1  1 -1]\\n [-1  1 -1]\\n [ 0 -1  0]]': [array([2, 0])], '[[ 1  1 -1]\\n [-1  1 -1]\\n [-1  0  0]]': [array([2, 2])], '[[ 1 -1 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[ 1 -1 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': [array([2, 2])], '[[ 1 -1 -1]\\n [ 0  1 -1]\\n [-1  1  0]]': [array([1, 0])], '[[ 1 -1 -1]\\n [ 1  1 -1]\\n [ 0 -1  0]]': [array([2, 2])], '[[ 1 -1 -1]\\n [ 1  1 -1]\\n [-1  0  0]]': [array([2, 2])], '[[ 1 -1 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': [array([2, 0])], '[[ 1 -1 -1]\\n [-1  1 -1]\\n [ 0  1  0]]': [array([2, 2])], '[[ 1 -1 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': [array([2, 2])], '[[ 1 -1 -1]\\n [-1  1 -1]\\n [ 1 -1  0]]': [array([2, 2])], '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 0  0  1]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 0  1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 0 -1  1]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 1  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': [array([2, 2])], '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 1 -1  1]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[-1  0 -1]\\n [ 0  1 -1]\\n [-1  0  1]]': [array([2, 1])], '[[-1  0 -1]\\n [ 0  1 -1]\\n [-1  1  1]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[-1  0 -1]\\n [ 1  1 -1]\\n [ 0  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[-1  0 -1]\\n [ 1  1 -1]\\n [ 0 -1  1]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[-1  0 -1]\\n [ 1  1 -1]\\n [ 1 -1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[-1  0 -1]\\n [ 1  1 -1]\\n [-1  0  1]]': [array([0, 1])], '[[-1  0 -1]\\n [-1  1 -1]\\n [ 0  0  1]]': [array([0, 1])], '[[-1  0 -1]\\n [-1  1 -1]\\n [ 0  1  1]]': [array([0, 1])], '[[-1  0 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': [array([0, 1])], '[[-1  0 -1]\\n [-1  1 -1]\\n [ 1  0  1]]': [array([0, 1])], '[[-1  0 -1]\\n [-1  1 -1]\\n [ 1  1  0]]': [array([2, 2])], '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 0 -1  0]]': array([[1, 0],\n",
              "        [1, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [2, 1],\n",
              "        [2, 2]]), '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 0 -1  1]]': [array([2, 0])], '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': [array([2, 2])], '[[-1  1 -1]\\n [ 0  1 -1]\\n [-1  0  0]]': [array([2, 1])], '[[-1  1 -1]\\n [ 0  1 -1]\\n [-1  0  1]]': [array([2, 1])], '[[-1  1 -1]\\n [ 0  1 -1]\\n [-1 -1  1]]': [array([1, 0])], '[[-1  1 -1]\\n [ 1  1 -1]\\n [ 0 -1  0]]': [array([2, 0])], '[[-1  1 -1]\\n [ 1  1 -1]\\n [-1  0  0]]': [array([2, 1])], '[[-1  1 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': [array([2, 1])], '[[-1  1 -1]\\n [-1  1 -1]\\n [ 0  0  1]]': [array([2, 0])], '[[-1  1 -1]\\n [-1  1 -1]\\n [ 0 -1  1]]': [array([2, 0])], '[[-1  1 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': [array([2, 1])]}"
            ]
          },
          "metadata": {},
          "execution_count": 800
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvFj0ROANJTA",
        "outputId": "0fd1517e-51fd-421a-9027-5588de149956"
      },
      "source": [
        "playerX.policy_trainer.Q"
      ],
      "execution_count": 801,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 0 -1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [-1  0  0]]': array([0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [-1  0  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [-1  1  0]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [-1 -1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 1  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 1  1 -1]\\n [-1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 1  1 -1]\\n [-1 -1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': array([0.90, 0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 0  0  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 0  1  0]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 0 -1  1]]': array([0.00, 0.90, 0.00, 0.00, 0.00, 0.00, 0.90, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 1 -1  0]]': array([0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 1 -1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [-1  1  1]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [ 0  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [ 0  1 -1]\\n [-1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [ 0  1 -1]\\n [-1 -1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [ 1  1 -1]\\n [-1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 0 -1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [-1  1 -1]\\n [-1  0  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [-1  1 -1]\\n [-1 -1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 0  0  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 0  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 0 -1  1]]': array([0.00, 0.00, 0.00, 0.90, 0.00, 0.00, 0.90, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 1 -1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [-1  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [-1  1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [ 0 -1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [ 1 -1  0]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [-1  0  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [-1  1 -1]\\n [ 0  1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90]),\n",
              " '[[ 0 -1 -1]\\n [-1  1 -1]\\n [ 1  0  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [-1  1 -1]\\n [ 1  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0 -1 -1]\\n [-1  1 -1]\\n [ 1 -1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 1  0 -1]\\n [ 0  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [ 0  1 -1]\\n [-1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [ 0  1 -1]\\n [-1 -1  0]]': array([0.00, 0.00, 0.00, 0.90, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 1  0 -1]\\n [ 1  1 -1]\\n [-1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [-1  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [-1  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  1 -1]\\n [ 0  1 -1]\\n [-1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 1  1 -1]\\n [-1  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  1 -1]\\n [-1  1 -1]\\n [-1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [ 0  1 -1]\\n [-1  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 1 -1 -1]\\n [ 1  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [ 1  1 -1]\\n [-1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90, 0.00, 0.00]),\n",
              " '[[ 1 -1 -1]\\n [-1  1 -1]\\n [ 0  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [-1  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([0.00, 0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 0  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 0  1  0]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 0 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 1 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [-1  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [-1  1  1]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 1  1 -1]\\n [ 0 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 1  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 1  1 -1]\\n [-1  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [-1  1 -1]\\n [ 0  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90, 0.00]),\n",
              " '[[-1  0 -1]\\n [-1  1 -1]\\n [ 0  1  1]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': array([0.00, 0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [-1  1 -1]\\n [ 1  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [-1  1 -1]\\n [ 1  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 0 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [-1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [-1  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [-1 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 1  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 1  1 -1]\\n [-1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [-1  1 -1]\\n [ 0  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [-1  1 -1]\\n [ 0 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00])}"
            ]
          },
          "metadata": {},
          "execution_count": 801
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8PJK7mKa4Yq"
      },
      "source": [
        "### Trained Q-values for Player O"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RyZifhaZ2MG",
        "outputId": "3aa09d34-89d9-44dd-9a86-ce2ec598254e"
      },
      "source": [
        "playerO.policy_trainer.Q"
      ],
      "execution_count": 802,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 0  0  1]]': array([0.90, 0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 0  1  0]]': array([0.90, 0.90, 0.00, 0.00, 0.00, 0.00, 0.90, 0.00, 1.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 0 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 1  0  0]]': array([0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [ 1 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [-1  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [-1  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  0 -1]\\n [ 0  1 -1]\\n [-1  1  1]]': array([0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 1  1 -1]\\n [ 0  0  0]]': array([0.90, 0.90, 0.00, 0.00, 0.00, 0.00, 0.90, 0.00, 1.00]),\n",
              " '[[ 0  0 -1]\\n [ 1  1 -1]\\n [ 0 -1  1]]': array([0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 1  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  0 -1]\\n [ 1  1 -1]\\n [-1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  0 -1]\\n [ 1  1 -1]\\n [-1  0  1]]': array([0.00, 0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [ 1  1 -1]\\n [-1  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 0  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 0  1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 1  0  1]]': array([0.00, 0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 1  1  0]]': array([0.00, 0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  0 -1]\\n [-1  1 -1]\\n [ 1 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([0.90, 0.00, 0.00, 0.90, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  1 -1]\\n [ 0  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [ 0  1 -1]\\n [ 0 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  1 -1]\\n [ 0  1 -1]\\n [-1  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [ 1  1 -1]\\n [ 0 -1  0]]': array([0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  1 -1]\\n [ 1  1 -1]\\n [-1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  1 -1]\\n [ 1  1 -1]\\n [-1 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 0  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 0 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': array([0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90, 1.00]),\n",
              " '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0  1 -1]\\n [-1  1 -1]\\n [ 1 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 0  0  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 0  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 0  1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 1  0  1]]': array([1.00, 0.00, 0.00, 0.90, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 1  1  0]]': array([1.00, 0.00, 0.00, 0.90, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 0  1 -1]\\n [ 1 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [ 0  0  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [ 0  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [ 0 -1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [ 1  0  0]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [ 1 -1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [-1  0  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [ 1  1 -1]\\n [-1  1  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [-1  1 -1]\\n [ 1  0  1]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 0 -1 -1]\\n [-1  1 -1]\\n [ 1  1  0]]': array([1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 1  0 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.90, 0.00, 0.00, 0.90, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [ 0  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 1  0 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.90, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 1  0 -1]\\n [ 0  1 -1]\\n [-1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [ 0  1 -1]\\n [-1  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 1  0 -1]\\n [ 1  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [ 1  1 -1]\\n [-1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [ 1  1 -1]\\n [-1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [-1  1 -1]\\n [ 0  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  0 -1]\\n [-1  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 1  1 -1]\\n [ 0  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  1 -1]\\n [ 0  1 -1]\\n [-1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  1 -1]\\n [ 1  1 -1]\\n [-1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1  1 -1]\\n [-1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[ 1  1 -1]\\n [-1  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [ 0  1 -1]\\n [ 0  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [ 0  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [ 1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [ 1  1 -1]\\n [-1  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[ 1 -1 -1]\\n [-1  1 -1]\\n [ 1  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 0  0  1]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 0  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 0  1  1]]': array([0.00, 1.00, 0.00, 0.90, 0.00, 0.00, 0.90, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 1  0  1]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 1  1  0]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [ 1 -1  1]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 0  1 -1]\\n [-1  1  1]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  0 -1]\\n [ 1  1 -1]\\n [ 0  0  1]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 1  1 -1]\\n [ 0  1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  0 -1]\\n [ 1  1 -1]\\n [ 0 -1  1]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 1  1 -1]\\n [ 1  0  0]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 1  1 -1]\\n [ 1 -1  1]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [ 1  1 -1]\\n [-1  1  1]]': array([0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  0 -1]\\n [-1  1 -1]\\n [ 0  1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 0  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 0 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  1 -1]\\n [ 0  1 -1]\\n [ 1 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 1  1 -1]\\n [ 0  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.90, 1.00]),\n",
              " '[[-1  1 -1]\\n [ 1  1 -1]\\n [ 0 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  1 -1]\\n [ 1  1 -1]\\n [ 0 -1  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [ 1  1 -1]\\n [ 1 -1  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  1 -1]\\n [ 1  1 -1]\\n [-1  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [-1  1 -1]\\n [ 0  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
              " '[[-1  1 -1]\\n [-1  1 -1]\\n [ 1  0  0]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00]),\n",
              " '[[-1  1 -1]\\n [-1  1 -1]\\n [ 1  0  1]]': array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00])}"
            ]
          },
          "metadata": {},
          "execution_count": 802
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrH2yc8nbAI-"
      },
      "source": [
        "# Testing Section\n",
        "Test the functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTUzmJs-09_u",
        "outputId": "2766d3d7-cdd5-4398-9bc2-50e2a34f2ba7"
      },
      "source": [
        "board=Board()\n",
        "print(board.checkWinner())\n",
        "\n",
        "board.state = np.array(((-1,-1,-1), (0,0,0), (0,0,0)))\n",
        "print(board.checkWinner())\n",
        "print(board.getStateHash())\n",
        "pos = board.getAvailablePos()\n",
        "print(\"getAvailablePos\\n\", pos)\n",
        "print(\"checkGameEnded\\n\", board.checkGameEnded())\n",
        "\n",
        "\n",
        "list1 = board.getAvailablePos()\n",
        "print(\"shape\\n\",list1.shape[0])\n",
        "ch1= np.random.choice(list1.shape[0])\n",
        "print(\"random.choice\\n\",ch1)\n",
        "print(\"random action\\n\", list1[ch1])\n",
        "\n",
        "board.state = np.array(((-1,0,0), (0,1,0), (0,0,-1)))\n",
        "print(board.getStateHash())\n",
        "\n",
        "board.state = np.array(((-1,1,1), (1,1,1), (1,-1,-1)))\n",
        "print(board.checkGameEnded())\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 803,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "-1\n",
            "[[-1 -1 -1]\n",
            " [ 0  0  0]\n",
            " [ 0  0  0]]\n",
            "getAvailablePos\n",
            " [[1 0]\n",
            " [1 1]\n",
            " [1 2]\n",
            " [2 0]\n",
            " [2 1]\n",
            " [2 2]]\n",
            "checkGameEnded\n",
            " True\n",
            "shape\n",
            " 6\n",
            "random.choice\n",
            " 0\n",
            "random action\n",
            " [1 0]\n",
            "[[-1  0  0]\n",
            " [ 0  1  0]\n",
            " [ 0  0 -1]]\n",
            "True\n"
          ]
        }
      ]
    }
  ]
}